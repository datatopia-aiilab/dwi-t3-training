{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8b8185d7",
   "metadata": {},
   "source": [
    "# DWI Artifact Segmentation Training on Google Colab\n",
    "\n",
    "This notebook trains an Attention U-Net model for artifact segmentation (red color: FF0000)\n",
    "\n",
    "## Setup Steps:\n",
    "1. Upload your data to Google Drive\n",
    "2. Mount Google Drive\n",
    "3. Configure paths and parameters\n",
    "4. Run training\n",
    "5. View results in MLflow"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cca3d296",
   "metadata": {},
   "source": [
    "## 1. Install Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29154958",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages\n",
    "!pip install -q torch torchvision\n",
    "!pip install -q opencv-python albumentations\n",
    "!pip install -q matplotlib tqdm\n",
    "!pip install -q mlflow"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79e1a41a",
   "metadata": {},
   "source": [
    "## 2. Mount Google Drive (if data is on Drive)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1a05ed5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "366c048c",
   "metadata": {},
   "source": [
    "## 3. Configuration\n",
    "\n",
    "### ðŸ“Œ à¸à¸³à¸«à¸™à¸”à¸„à¹ˆà¸²à¸—à¸µà¹ˆà¸™à¸µà¹ˆ!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32314bb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from pathlib import Path\n",
    "\n",
    "# ==================== âš™ï¸ CONFIGURE HERE ====================\n",
    "\n",
    "# ðŸ“‚ Base Path (à¸›à¸£à¸±à¸šà¹ƒà¸«à¹‰à¸•à¸£à¸‡à¸à¸±à¸š working directory à¸‚à¸­à¸‡à¸„à¸¸à¸“)\n",
    "BASE_PATH = Path(\"/content/drive/MyDrive/AiiLAB/_ProjectCommercial/_DWI-DBMSPattaya\")\n",
    "\n",
    "# ðŸ“‚ Data Paths - à¹ƒà¸Šà¹‰ relative path à¸ˆà¸²à¸ BASE_PATH\n",
    "MASKS_DIR = BASE_PATH / \"T4-ARTFSEG/Dataset-ArfifactLabel\"  # Path to mask folder\n",
    "ORIGINAL_DIR = BASE_PATH / \"T2-ARTIFACT_CLASSIFY/dataset-t2-24jul/Artifact\"  # Path to original images folder\n",
    "MODELS_DIR = BASE_PATH / \"T4-ARTFSEG/models\"  # Where to save models\n",
    "\n",
    "# ðŸŽ¨ Artifact Color (Red: FF0000)\n",
    "ARTIFACT_COLOR = [255, 0, 0]  # RGB\n",
    "\n",
    "# ðŸ“ Image Size\n",
    "IMAGE_SIZE = (384, 384)\n",
    "\n",
    "# ðŸ“Š Data Split Ratio\n",
    "TRAIN_RATIO = 0.80\n",
    "VAL_RATIO = 0.15\n",
    "TEST_RATIO = 0.05\n",
    "RANDOM_SEED = 42\n",
    "\n",
    "# ðŸ—ï¸ Model Architecture\n",
    "IN_CHANNELS = 3      # RGB input\n",
    "OUT_CHANNELS = 1     # Binary segmentation\n",
    "BASE_CHANNELS = 64   # Model size (64=~31M params, 32=~8M params)\n",
    "\n",
    "# ðŸŽ¯ Training Parameters\n",
    "EPOCHS = 100\n",
    "BATCH_SIZE = 16  # à¸¥à¸”à¸–à¹‰à¸² Out of Memory\n",
    "LEARNING_RATE = 1e-4\n",
    "WEIGHT_DECAY = 1e-5\n",
    "EARLY_STOP_PATIENCE = 20\n",
    "\n",
    "# ðŸ”„ Augmentation\n",
    "USE_AUGMENTATION = False  # True = enable augmentation\n",
    "AUG_HFLIP_PROB = 0.3\n",
    "AUG_ROTATE_LIMIT = 10\n",
    "AUG_ROTATE_PROB = 0.25\n",
    "AUG_BRIGHTNESS_LIMIT = 0.1\n",
    "AUG_BRIGHTNESS_PROB = 0.2\n",
    "\n",
    "# ðŸ“ MLflow\n",
    "MLFLOW_EXPERIMENT_NAME = \"DWI-Artifact-Colab\"\n",
    "\n",
    "# ðŸ’» Device\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "USE_AMP = True if torch.cuda.is_available() else False\n",
    "\n",
    "# ==================== END CONFIGURATION ====================\n",
    "\n",
    "# Create directories\n",
    "MODELS_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Print configuration\n",
    "print(\"=\"*60)\n",
    "print(\"ðŸ“‹ CONFIGURATION\")\n",
    "print(\"=\"*60)\n",
    "print(f\"Base Path: {BASE_PATH}\")\n",
    "print(f\"Masks Dir: {MASKS_DIR}\")\n",
    "print(f\"  â†’ Exists: {MASKS_DIR.exists()}\")\n",
    "print(f\"Original Dir: {ORIGINAL_DIR}\")\n",
    "print(f\"  â†’ Exists: {ORIGINAL_DIR.exists()}\")\n",
    "print(f\"Models Dir: {MODELS_DIR}\")\n",
    "print(f\"\\nImage Size: {IMAGE_SIZE}\")\n",
    "print(f\"Artifact Color: RGB{ARTIFACT_COLOR}\")\n",
    "print(f\"\\nSplit: {TRAIN_RATIO:.0%} train / {VAL_RATIO:.0%} val / {TEST_RATIO:.0%} test\")\n",
    "print(f\"\\nModel: Attention U-Net\")\n",
    "print(f\"Base Channels: {BASE_CHANNELS}\")\n",
    "print(f\"\\nTraining:\")\n",
    "print(f\"  Epochs: {EPOCHS}\")\n",
    "print(f\"  Batch Size: {BATCH_SIZE}\")\n",
    "print(f\"  Learning Rate: {LEARNING_RATE}\")\n",
    "print(f\"  Device: {DEVICE}\")\n",
    "print(f\"  Mixed Precision: {USE_AMP}\")\n",
    "print(f\"\\nAugmentation: {'ENABLED âœ“' if USE_AUGMENTATION else 'DISABLED âœ—'}\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# à¹à¸ªà¸”à¸‡à¹„à¸Ÿà¸¥à¹Œà¸•à¸±à¸§à¸­à¸¢à¹ˆà¸²à¸‡à¹ƒà¸™ MASKS_DIR\n",
    "if MASKS_DIR.exists():\n",
    "    import glob\n",
    "    sample_files = glob.glob(str(MASKS_DIR / \"*.png\"))[:5]\n",
    "    if sample_files:\n",
    "        print(f\"\\nâœ“ Found PNG files in MASKS_DIR:\")\n",
    "        for f in sample_files:\n",
    "            print(f\"  - {Path(f).name}\")\n",
    "    else:\n",
    "        print(f\"\\nâš ï¸ No PNG files found in {MASKS_DIR}\")\n",
    "        print(\"Please check the path!\")\n",
    "else:\n",
    "    print(f\"\\nâŒ MASKS_DIR does not exist: {MASKS_DIR}\")\n",
    "    print(\"Please check the path!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92a78abe",
   "metadata": {},
   "source": [
    "## 4. Import Model Definition\n",
    "\n",
    "Copy from model.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3670ece",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class ConvBlock(nn.Module):\n",
    "    \"\"\"Standard Conv Block: Conv -> BN -> ReLU -> Conv -> BN -> ReLU\"\"\"\n",
    "    \n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super(ConvBlock, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1)\n",
    "        self.bn1 = nn.BatchNorm2d(out_channels)\n",
    "        self.conv2 = nn.Conv2d(out_channels, out_channels, kernel_size=3, padding=1)\n",
    "        self.bn2 = nn.BatchNorm2d(out_channels)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.relu(self.bn1(self.conv1(x)))\n",
    "        x = self.relu(self.bn2(self.conv2(x)))\n",
    "        return x\n",
    "\n",
    "\n",
    "class AttentionGate(nn.Module):\n",
    "    \"\"\"Attention Gate for focusing on relevant features\"\"\"\n",
    "    \n",
    "    def __init__(self, F_g, F_l, F_int):\n",
    "        super(AttentionGate, self).__init__()\n",
    "        \n",
    "        self.W_g = nn.Sequential(\n",
    "            nn.Conv2d(F_g, F_int, kernel_size=1, stride=1, padding=0, bias=True),\n",
    "            nn.BatchNorm2d(F_int)\n",
    "        )\n",
    "        \n",
    "        self.W_x = nn.Sequential(\n",
    "            nn.Conv2d(F_l, F_int, kernel_size=1, stride=1, padding=0, bias=True),\n",
    "            nn.BatchNorm2d(F_int)\n",
    "        )\n",
    "        \n",
    "        self.psi = nn.Sequential(\n",
    "            nn.Conv2d(F_int, 1, kernel_size=1, stride=1, padding=0, bias=True),\n",
    "            nn.BatchNorm2d(1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "        \n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "    \n",
    "    def forward(self, g, x):\n",
    "        g1 = self.W_g(g)\n",
    "        x1 = self.W_x(x)\n",
    "        psi = self.relu(g1 + x1)\n",
    "        psi = self.psi(psi)\n",
    "        return x * psi\n",
    "\n",
    "\n",
    "class AttentionUNet(nn.Module):\n",
    "    \"\"\"Attention U-Net for artifact segmentation\"\"\"\n",
    "    \n",
    "    def __init__(self, in_channels=3, out_channels=1, base_channels=64):\n",
    "        super(AttentionUNet, self).__init__()\n",
    "        \n",
    "        # Encoder\n",
    "        self.enc1 = ConvBlock(in_channels, base_channels)\n",
    "        self.pool1 = nn.MaxPool2d(2)\n",
    "        \n",
    "        self.enc2 = ConvBlock(base_channels, base_channels * 2)\n",
    "        self.pool2 = nn.MaxPool2d(2)\n",
    "        \n",
    "        self.enc3 = ConvBlock(base_channels * 2, base_channels * 4)\n",
    "        self.pool3 = nn.MaxPool2d(2)\n",
    "        \n",
    "        self.enc4 = ConvBlock(base_channels * 4, base_channels * 8)\n",
    "        self.pool4 = nn.MaxPool2d(2)\n",
    "        \n",
    "        # Bottleneck\n",
    "        self.bottleneck = ConvBlock(base_channels * 8, base_channels * 16)\n",
    "        \n",
    "        # Decoder with Attention Gates\n",
    "        self.up4 = nn.ConvTranspose2d(base_channels * 16, base_channels * 8, kernel_size=2, stride=2)\n",
    "        self.att4 = AttentionGate(F_g=base_channels * 8, F_l=base_channels * 8, F_int=base_channels * 4)\n",
    "        self.dec4 = ConvBlock(base_channels * 16, base_channels * 8)\n",
    "        \n",
    "        self.up3 = nn.ConvTranspose2d(base_channels * 8, base_channels * 4, kernel_size=2, stride=2)\n",
    "        self.att3 = AttentionGate(F_g=base_channels * 4, F_l=base_channels * 4, F_int=base_channels * 2)\n",
    "        self.dec3 = ConvBlock(base_channels * 8, base_channels * 4)\n",
    "        \n",
    "        self.up2 = nn.ConvTranspose2d(base_channels * 4, base_channels * 2, kernel_size=2, stride=2)\n",
    "        self.att2 = AttentionGate(F_g=base_channels * 2, F_l=base_channels * 2, F_int=base_channels)\n",
    "        self.dec2 = ConvBlock(base_channels * 4, base_channels * 2)\n",
    "        \n",
    "        self.up1 = nn.ConvTranspose2d(base_channels * 2, base_channels, kernel_size=2, stride=2)\n",
    "        self.att1 = AttentionGate(F_g=base_channels, F_l=base_channels, F_int=base_channels // 2)\n",
    "        self.dec1 = ConvBlock(base_channels * 2, base_channels)\n",
    "        \n",
    "        # Output\n",
    "        self.out = nn.Conv2d(base_channels, out_channels, kernel_size=1)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # Encoder\n",
    "        e1 = self.enc1(x)\n",
    "        e2 = self.enc2(self.pool1(e1))\n",
    "        e3 = self.enc3(self.pool2(e2))\n",
    "        e4 = self.enc4(self.pool3(e3))\n",
    "        \n",
    "        # Bottleneck\n",
    "        b = self.bottleneck(self.pool4(e4))\n",
    "        \n",
    "        # Decoder with Attention\n",
    "        d4 = self.up4(b)\n",
    "        e4_att = self.att4(g=d4, x=e4)\n",
    "        d4 = torch.cat([e4_att, d4], dim=1)\n",
    "        d4 = self.dec4(d4)\n",
    "        \n",
    "        d3 = self.up3(d4)\n",
    "        e3_att = self.att3(g=d3, x=e3)\n",
    "        d3 = torch.cat([e3_att, d3], dim=1)\n",
    "        d3 = self.dec3(d3)\n",
    "        \n",
    "        d2 = self.up2(d3)\n",
    "        e2_att = self.att2(g=d2, x=e2)\n",
    "        d2 = torch.cat([e2_att, d2], dim=1)\n",
    "        d2 = self.dec2(d2)\n",
    "        \n",
    "        d1 = self.up1(d2)\n",
    "        e1_att = self.att1(g=d1, x=e1)\n",
    "        d1 = torch.cat([e1_att, d1], dim=1)\n",
    "        d1 = self.dec1(d1)\n",
    "        \n",
    "        # Output\n",
    "        out = self.out(d1)\n",
    "        return torch.sigmoid(out)\n",
    "\n",
    "\n",
    "def count_parameters(model):\n",
    "    \"\"\"Count total and trainable parameters\"\"\"\n",
    "    total = sum(p.numel() for p in model.parameters())\n",
    "    trainable = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "    return total, trainable\n",
    "\n",
    "\n",
    "print(\"âœ… Model classes loaded successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "597cda73",
   "metadata": {},
   "source": [
    "## 5. Import Training Functions\n",
    "\n",
    "Copy from train.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85eb2a24",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import json\n",
    "import tempfile\n",
    "from datetime import datetime\n",
    "\n",
    "import numpy as np\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import albumentations as A\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "\n",
    "import mlflow\n",
    "import mlflow.pytorch\n",
    "\n",
    "\n",
    "# ==================== Dataset ====================\n",
    "\n",
    "class ArtifactDataset(Dataset):\n",
    "    \"\"\"Simple Artifact Dataset with RGB images\"\"\"\n",
    "    \n",
    "    def __init__(self, images, masks, transform=None):\n",
    "        self.images = images\n",
    "        self.masks = masks\n",
    "        self.transform = transform\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.images)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        image = self.images[idx]  # (H, W, 3) - RGB\n",
    "        mask = self.masks[idx]    # (H, W)\n",
    "        \n",
    "        if self.transform:\n",
    "            augmented = self.transform(image=image, mask=mask)\n",
    "            image = augmented['image']\n",
    "            mask = augmented['mask']\n",
    "            mask = mask.unsqueeze(0).float()\n",
    "        else:\n",
    "            image = torch.from_numpy(image).permute(2, 0, 1).float() / 255.0\n",
    "            mask = torch.from_numpy(mask).unsqueeze(0).float()\n",
    "        \n",
    "        return image, mask\n",
    "\n",
    "\n",
    "# ==================== Loss Function ====================\n",
    "\n",
    "class DiceLoss(nn.Module):\n",
    "    \"\"\"Simple Dice Loss\"\"\"\n",
    "    \n",
    "    def __init__(self, smooth=1e-6):\n",
    "        super(DiceLoss, self).__init__()\n",
    "        self.smooth = smooth\n",
    "    \n",
    "    def forward(self, pred, target):\n",
    "        pred = pred.view(-1)\n",
    "        target = target.view(-1)\n",
    "        \n",
    "        intersection = (pred * target).sum()\n",
    "        dice = (2. * intersection + self.smooth) / (pred.sum() + target.sum() + self.smooth)\n",
    "        \n",
    "        return 1 - dice\n",
    "\n",
    "\n",
    "def calculate_dice_score(pred, target):\n",
    "    \"\"\"Calculate Dice score (metric)\"\"\"\n",
    "    pred = pred.view(-1)\n",
    "    target = target.view(-1)\n",
    "    \n",
    "    intersection = (pred * target).sum()\n",
    "    dice = (2. * intersection + 1e-6) / (pred.sum() + target.sum() + 1e-6)\n",
    "    \n",
    "    return dice.item()\n",
    "\n",
    "\n",
    "def calculate_iou(pred, target):\n",
    "    \"\"\"Calculate IoU (Jaccard Index)\"\"\"\n",
    "    pred = pred.view(-1)\n",
    "    target = target.view(-1)\n",
    "    \n",
    "    intersection = (pred * target).sum()\n",
    "    union = pred.sum() + target.sum() - intersection\n",
    "    iou = (intersection + 1e-6) / (union + 1e-6)\n",
    "    \n",
    "    return iou.item()\n",
    "\n",
    "\n",
    "print(\"âœ… Training functions loaded successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ada6973",
   "metadata": {},
   "source": [
    "## 6. Data Loading Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2afd1297",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_red_mask(mask_rgb, artifact_color=[255, 0, 0], tolerance=10):\n",
    "    \"\"\"Extract binary mask from red artifacts in RGB mask\"\"\"\n",
    "    lower = np.array([max(0, c - tolerance) for c in artifact_color])\n",
    "    upper = np.array([min(255, c + tolerance) for c in artifact_color])\n",
    "    \n",
    "    binary_mask = cv2.inRange(mask_rgb, lower, upper)\n",
    "    binary_mask = (binary_mask > 0).astype(np.float32)\n",
    "    \n",
    "    return binary_mask\n",
    "\n",
    "\n",
    "def load_and_preprocess_data():\n",
    "    \"\"\"Load PNG files and prepare datasets\"\"\"\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"ðŸ“‚ Loading and Preprocessing Data\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    # Find all mask files\n",
    "    mask_files = sorted(glob.glob(str(MASKS_DIR / \"*.png\")))\n",
    "    print(f\"Found {len(mask_files)} mask files in {MASKS_DIR}\")\n",
    "    \n",
    "    if len(mask_files) == 0:\n",
    "        raise FileNotFoundError(f\"No .png files found in {MASKS_DIR}\")\n",
    "    \n",
    "    # Load all images and extract masks\n",
    "    all_images = []\n",
    "    all_masks = []\n",
    "    skipped = 0\n",
    "    \n",
    "    print(\"\\nLoading PNG files...\")\n",
    "    for mask_path in tqdm(mask_files, desc=\"Loading\"):\n",
    "        mask_name = Path(mask_path).name\n",
    "        original_path = ORIGINAL_DIR / mask_name\n",
    "        \n",
    "        if not original_path.exists():\n",
    "            print(f\"Warning: Original not found for {mask_name}, skipping...\")\n",
    "            skipped += 1\n",
    "            continue\n",
    "        \n",
    "        # Load images\n",
    "        original_img = cv2.imread(str(original_path))\n",
    "        mask_img = cv2.imread(str(mask_path))\n",
    "        \n",
    "        if original_img is None or mask_img is None:\n",
    "            print(f\"Warning: Failed to load {mask_name}, skipping...\")\n",
    "            skipped += 1\n",
    "            continue\n",
    "        \n",
    "        # Convert BGR to RGB\n",
    "        original_img = cv2.cvtColor(original_img, cv2.COLOR_BGR2RGB)\n",
    "        mask_img = cv2.cvtColor(mask_img, cv2.COLOR_BGR2RGB)\n",
    "        \n",
    "        # Extract red artifact mask\n",
    "        binary_mask = extract_red_mask(mask_img, ARTIFACT_COLOR)\n",
    "        \n",
    "        # Skip if no artifact found\n",
    "        if binary_mask.sum() == 0:\n",
    "            skipped += 1\n",
    "            continue\n",
    "        \n",
    "        # Resize\n",
    "        original_img = cv2.resize(original_img, IMAGE_SIZE)\n",
    "        binary_mask = cv2.resize(binary_mask, IMAGE_SIZE, interpolation=cv2.INTER_NEAREST)\n",
    "        binary_mask = (binary_mask > 0.5).astype(np.float32)\n",
    "        \n",
    "        all_images.append(original_img)\n",
    "        all_masks.append(binary_mask)\n",
    "    \n",
    "    # Convert to numpy arrays\n",
    "    all_images = np.array(all_images, dtype=np.uint8)\n",
    "    all_masks = np.array(all_masks, dtype=np.float32)\n",
    "    \n",
    "    print(f\"\\nTotal images loaded: {len(all_images)}\")\n",
    "    print(f\"Skipped: {skipped}\")\n",
    "    print(f\"Image shape: {all_images.shape}\")\n",
    "    print(f\"Mask shape: {all_masks.shape}\")\n",
    "    \n",
    "    if len(all_images) == 0:\n",
    "        raise ValueError(\"No valid images loaded. Check your data!\")\n",
    "    \n",
    "    # Split into train/val/test\n",
    "    np.random.seed(RANDOM_SEED)\n",
    "    indices = np.random.permutation(len(all_images))\n",
    "    \n",
    "    n_train = int(len(indices) * TRAIN_RATIO)\n",
    "    n_val = int(len(indices) * VAL_RATIO)\n",
    "    \n",
    "    train_idx = indices[:n_train]\n",
    "    val_idx = indices[n_train:n_train + n_val]\n",
    "    test_idx = indices[n_train + n_val:]\n",
    "    \n",
    "    train_images = all_images[train_idx]\n",
    "    train_masks = all_masks[train_idx]\n",
    "    val_images = all_images[val_idx]\n",
    "    val_masks = all_masks[val_idx]\n",
    "    test_images = all_images[test_idx]\n",
    "    test_masks = all_masks[test_idx]\n",
    "    \n",
    "    print(f\"\\nSplit:\")\n",
    "    print(f\"  Train: {len(train_images)} images\")\n",
    "    print(f\"  Val: {len(val_images)} images\")\n",
    "    print(f\"  Test: {len(test_images)} images\")\n",
    "    \n",
    "    # Create augmentation transforms\n",
    "    if USE_AUGMENTATION:\n",
    "        print(\"\\nâœ“ Augmentation ENABLED for training\")\n",
    "        train_transform = A.Compose([\n",
    "            A.HorizontalFlip(p=AUG_HFLIP_PROB),\n",
    "            A.Rotate(limit=AUG_ROTATE_LIMIT, p=AUG_ROTATE_PROB),\n",
    "            A.RandomBrightnessContrast(\n",
    "                brightness_limit=AUG_BRIGHTNESS_LIMIT,\n",
    "                contrast_limit=AUG_BRIGHTNESS_LIMIT,\n",
    "                p=AUG_BRIGHTNESS_PROB\n",
    "            ),\n",
    "            A.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "            ToTensorV2()\n",
    "        ])\n",
    "    else:\n",
    "        print(\"\\nâœ— Augmentation DISABLED\")\n",
    "        train_transform = A.Compose([\n",
    "            A.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "            ToTensorV2()\n",
    "        ])\n",
    "    \n",
    "    val_transform = A.Compose([\n",
    "        A.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "        ToTensorV2()\n",
    "    ])\n",
    "    \n",
    "    # Create datasets\n",
    "    train_dataset = ArtifactDataset(train_images, train_masks, transform=train_transform)\n",
    "    val_dataset = ArtifactDataset(val_images, val_masks, transform=val_transform)\n",
    "    test_dataset = ArtifactDataset(test_images, test_masks, transform=val_transform)\n",
    "    \n",
    "    # Create dataloaders\n",
    "    train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=2, pin_memory=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=2, pin_memory=True)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=1, shuffle=False, num_workers=2, pin_memory=True)\n",
    "    \n",
    "    print(\"=\"*60 + \"\\n\")\n",
    "    \n",
    "    return train_loader, val_loader, test_loader\n",
    "\n",
    "\n",
    "print(\"âœ… Data loading functions ready!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20c5c484",
   "metadata": {},
   "source": [
    "## 7. Training Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77fad33e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_one_epoch(model, dataloader, criterion, optimizer, scaler, device):\n",
    "    \"\"\"Train for one epoch\"\"\"\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    running_dice = 0.0\n",
    "    \n",
    "    pbar = tqdm(dataloader, desc=\"Training\")\n",
    "    for images, masks in pbar:\n",
    "        images = images.to(device)\n",
    "        masks = masks.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        if USE_AMP:\n",
    "            with torch.amp.autocast('cuda'):\n",
    "                outputs = model(images)\n",
    "                loss = criterion(outputs, masks)\n",
    "            \n",
    "            scaler.scale(loss).backward()\n",
    "            scaler.step(optimizer)\n",
    "            scaler.update()\n",
    "        else:\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, masks)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            preds = (outputs > 0.5).float()\n",
    "            dice = calculate_dice_score(preds, masks)\n",
    "        \n",
    "        running_loss += loss.item()\n",
    "        running_dice += dice\n",
    "        \n",
    "        pbar.set_postfix({'loss': f'{loss.item():.4f}', 'dice': f'{dice:.4f}'})\n",
    "    \n",
    "    avg_loss = running_loss / len(dataloader)\n",
    "    avg_dice = running_dice / len(dataloader)\n",
    "    \n",
    "    return avg_loss, avg_dice\n",
    "\n",
    "\n",
    "def validate(model, dataloader, criterion, device):\n",
    "    \"\"\"Validate the model\"\"\"\n",
    "    model.eval()\n",
    "    running_loss = 0.0\n",
    "    running_dice = 0.0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        pbar = tqdm(dataloader, desc=\"Validation\")\n",
    "        for images, masks in pbar:\n",
    "            images = images.to(device)\n",
    "            masks = masks.to(device)\n",
    "            \n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, masks)\n",
    "            \n",
    "            preds = (outputs > 0.5).float()\n",
    "            dice = calculate_dice_score(preds, masks)\n",
    "            \n",
    "            running_loss += loss.item()\n",
    "            running_dice += dice\n",
    "            \n",
    "            pbar.set_postfix({'loss': f'{loss.item():.4f}', 'dice': f'{dice:.4f}'})\n",
    "    \n",
    "    avg_loss = running_loss / len(dataloader)\n",
    "    avg_dice = running_dice / len(dataloader)\n",
    "    \n",
    "    return avg_loss, avg_dice\n",
    "\n",
    "\n",
    "print(\"âœ… Training functions ready!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcadf57f",
   "metadata": {},
   "source": [
    "## 8. Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3436f3e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set random seeds\n",
    "torch.manual_seed(RANDOM_SEED)\n",
    "np.random.seed(RANDOM_SEED)\n",
    "\n",
    "# Load data\n",
    "train_loader, val_loader, test_loader = load_and_preprocess_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e608fe29",
   "metadata": {},
   "source": [
    "## 9. Create Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc2dadbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*60)\n",
    "print(\"ðŸ—ï¸  Creating Model\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "model = AttentionUNet(\n",
    "    in_channels=IN_CHANNELS,\n",
    "    out_channels=OUT_CHANNELS,\n",
    "    base_channels=BASE_CHANNELS\n",
    ").to(DEVICE)\n",
    "\n",
    "total_params, trainable_params = count_parameters(model)\n",
    "print(f\"Total parameters: {total_params:,} ({total_params/1e6:.1f}M)\")\n",
    "print(f\"Trainable parameters: {trainable_params:,} ({trainable_params/1e6:.1f}M)\")\n",
    "print(\"=\"*60 + \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f81f04f4",
   "metadata": {},
   "source": [
    "## 10. Setup Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c64a9845",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loss, optimizer, scheduler\n",
    "criterion = DiceLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE, weight_decay=WEIGHT_DECAY)\n",
    "scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='max', factor=0.5, patience=5)\n",
    "\n",
    "# Mixed precision scaler\n",
    "scaler = torch.amp.GradScaler('cuda') if USE_AMP else None\n",
    "\n",
    "# Setup MLflow\n",
    "mlflow.set_experiment(MLFLOW_EXPERIMENT_NAME)\n",
    "\n",
    "print(\"âœ… Training setup complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f1b4dd3",
   "metadata": {},
   "source": [
    "## 11. Start Training ðŸš€"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e84246b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "run_name = f\"artifact_colab_{datetime.now():%Y%m%d_%H%M%S}\"\n",
    "\n",
    "with mlflow.start_run(run_name=run_name):\n",
    "    print(\"=\"*60)\n",
    "    print(\"ðŸ”¬ MLflow Tracking Initialized\")\n",
    "    print(\"=\"*60)\n",
    "    print(f\"Experiment: {MLFLOW_EXPERIMENT_NAME}\")\n",
    "    print(f\"Run Name: {run_name}\")\n",
    "    print(\"=\"*60 + \"\\n\")\n",
    "    \n",
    "    # Log parameters\n",
    "    mlflow.log_param(\"image_size\", IMAGE_SIZE)\n",
    "    mlflow.log_param(\"batch_size\", BATCH_SIZE)\n",
    "    mlflow.log_param(\"learning_rate\", LEARNING_RATE)\n",
    "    mlflow.log_param(\"epochs\", EPOCHS)\n",
    "    mlflow.log_param(\"base_channels\", BASE_CHANNELS)\n",
    "    mlflow.log_param(\"train_samples\", len(train_loader.dataset))\n",
    "    mlflow.log_param(\"val_samples\", len(val_loader.dataset))\n",
    "    mlflow.log_param(\"test_samples\", len(test_loader.dataset))\n",
    "    mlflow.log_param(\"model_params\", total_params)\n",
    "    mlflow.log_param(\"use_augmentation\", USE_AUGMENTATION)\n",
    "    mlflow.log_param(\"artifact_color\", ARTIFACT_COLOR)\n",
    "    mlflow.log_param(\"platform\", \"Google Colab\")\n",
    "    \n",
    "    # Training loop\n",
    "    print(\"=\"*60)\n",
    "    print(\"ðŸš€ Starting Training\")\n",
    "    print(\"=\"*60 + \"\\n\")\n",
    "    \n",
    "    best_dice = 0.0\n",
    "    patience_counter = 0\n",
    "    history = {'train_loss': [], 'train_dice': [], 'val_loss': [], 'val_dice': []}\n",
    "    \n",
    "    for epoch in range(EPOCHS):\n",
    "        print(f\"\\nEpoch {epoch+1}/{EPOCHS}\")\n",
    "        print(\"-\" * 60)\n",
    "        \n",
    "        # Train\n",
    "        train_loss, train_dice = train_one_epoch(model, train_loader, criterion, optimizer, scaler, DEVICE)\n",
    "        \n",
    "        # Validate\n",
    "        val_loss, val_dice = validate(model, val_loader, criterion, DEVICE)\n",
    "        \n",
    "        # Update scheduler\n",
    "        scheduler.step(val_dice)\n",
    "        current_lr = optimizer.param_groups[0]['lr']\n",
    "        \n",
    "        # Save history\n",
    "        history['train_loss'].append(train_loss)\n",
    "        history['train_dice'].append(train_dice)\n",
    "        history['val_loss'].append(val_loss)\n",
    "        history['val_dice'].append(val_dice)\n",
    "        \n",
    "        # Log to MLflow\n",
    "        mlflow.log_metric(\"train_loss\", train_loss, step=epoch)\n",
    "        mlflow.log_metric(\"train_dice\", train_dice, step=epoch)\n",
    "        mlflow.log_metric(\"val_loss\", val_loss, step=epoch)\n",
    "        mlflow.log_metric(\"val_dice\", val_dice, step=epoch)\n",
    "        mlflow.log_metric(\"learning_rate\", current_lr, step=epoch)\n",
    "        \n",
    "        # Print summary\n",
    "        print(f\"\\nResults:\")\n",
    "        print(f\"  Train Loss: {train_loss:.4f} | Train Dice: {train_dice:.4f}\")\n",
    "        print(f\"  Val Loss: {val_loss:.4f} | Val Dice: {val_dice:.4f}\")\n",
    "        print(f\"  Learning Rate: {current_lr:.6f}\")\n",
    "        \n",
    "        # Save best model\n",
    "        if val_dice > best_dice:\n",
    "            best_dice = val_dice\n",
    "            patience_counter = 0\n",
    "            torch.save(model.state_dict(), MODELS_DIR / \"best_model.pth\")\n",
    "            print(f\"  âœ… New best model saved! (Dice: {best_dice:.4f})\")\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "            print(f\"  No improvement ({patience_counter}/{EARLY_STOP_PATIENCE})\")\n",
    "        \n",
    "        # Early stopping\n",
    "        if patience_counter >= EARLY_STOP_PATIENCE:\n",
    "            print(f\"\\nâ¹ï¸  Early stopping triggered after {epoch+1} epochs\")\n",
    "            break\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"âœ… Training Completed!\")\n",
    "    print(\"=\"*60)\n",
    "    print(f\"Best Val Dice: {best_dice:.4f}\")\n",
    "    print(\"=\"*60 + \"\\n\")\n",
    "    \n",
    "    # Log best model\n",
    "    mlflow.log_artifact(str(MODELS_DIR / \"best_model.pth\"))\n",
    "    mlflow.log_metric(\"best_val_dice\", best_dice)\n",
    "\n",
    "print(\"\\nðŸŽ‰ Training finished!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "627c7081",
   "metadata": {},
   "source": [
    "## 12. Plot Training Curves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d514653",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 2, figsize=(15, 5))\n",
    "\n",
    "epochs = range(1, len(history['train_loss']) + 1)\n",
    "\n",
    "# Loss\n",
    "axes[0].plot(epochs, history['train_loss'], 'b-', label='Train Loss')\n",
    "axes[0].plot(epochs, history['val_loss'], 'r-', label='Val Loss')\n",
    "axes[0].set_xlabel('Epoch')\n",
    "axes[0].set_ylabel('Loss')\n",
    "axes[0].set_title('Training and Validation Loss')\n",
    "axes[0].legend()\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# Dice\n",
    "axes[1].plot(epochs, history['train_dice'], 'b-', label='Train Dice')\n",
    "axes[1].plot(epochs, history['val_dice'], 'r-', label='Val Dice')\n",
    "axes[1].set_xlabel('Epoch')\n",
    "axes[1].set_ylabel('Dice Score')\n",
    "axes[1].set_title('Training and Validation Dice Score')\n",
    "axes[1].legend()\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(MODELS_DIR / 'training_curves.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b05f3d4f",
   "metadata": {},
   "source": [
    "## 13. Evaluate on Test Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c018d7dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load best model\n",
    "model.load_state_dict(torch.load(MODELS_DIR / \"best_model.pth\"))\n",
    "model.eval()\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"ðŸ“Š Evaluating on Test Set\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "all_dice = []\n",
    "all_iou = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for images, masks in tqdm(test_loader, desc=\"Testing\"):\n",
    "        images = images.to(DEVICE)\n",
    "        masks = masks.to(DEVICE)\n",
    "        \n",
    "        outputs = model(images)\n",
    "        preds = (outputs > 0.5).float()\n",
    "        \n",
    "        dice = calculate_dice_score(preds, masks)\n",
    "        iou = calculate_iou(preds, masks)\n",
    "        \n",
    "        all_dice.append(dice)\n",
    "        all_iou.append(iou)\n",
    "\n",
    "mean_dice = np.mean(all_dice)\n",
    "std_dice = np.std(all_dice)\n",
    "mean_iou = np.mean(all_iou)\n",
    "std_iou = np.std(all_iou)\n",
    "\n",
    "print(f\"\\nTest Results:\")\n",
    "print(f\"  Dice Score: {mean_dice:.4f} Â± {std_dice:.4f}\")\n",
    "print(f\"  IoU Score: {mean_iou:.4f} Â± {std_iou:.4f}\")\n",
    "print(\"=\"*60 + \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58875ead",
   "metadata": {},
   "source": [
    "## 14. Visualize Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb897e55",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get a few test samples\n",
    "model.eval()\n",
    "test_iter = iter(test_loader)\n",
    "\n",
    "fig, axes = plt.subplots(3, 3, figsize=(15, 15))\n",
    "\n",
    "for idx in range(min(3, len(test_loader))):\n",
    "    images, masks = next(test_iter)\n",
    "    images = images.to(DEVICE)\n",
    "    masks = masks.to(DEVICE)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = model(images)\n",
    "        preds = (outputs > 0.5).float()\n",
    "    \n",
    "    # Denormalize image\n",
    "    img_np = images[0].cpu().numpy().transpose(1, 2, 0)\n",
    "    img_np = img_np * np.array([0.229, 0.224, 0.225]) + np.array([0.485, 0.456, 0.406])\n",
    "    img_np = np.clip(img_np, 0, 1)\n",
    "    \n",
    "    mask_np = masks[0, 0].cpu().numpy()\n",
    "    pred_np = preds[0, 0].cpu().numpy()\n",
    "    \n",
    "    dice = calculate_dice_score(preds, masks)\n",
    "    \n",
    "    # Image\n",
    "    axes[idx, 0].imshow(img_np)\n",
    "    axes[idx, 0].set_title('Input Image')\n",
    "    axes[idx, 0].axis('off')\n",
    "    \n",
    "    # Ground Truth\n",
    "    axes[idx, 1].imshow(img_np)\n",
    "    axes[idx, 1].imshow(mask_np, cmap='Reds', alpha=0.5)\n",
    "    axes[idx, 1].set_title('Ground Truth')\n",
    "    axes[idx, 1].axis('off')\n",
    "    \n",
    "    # Prediction\n",
    "    axes[idx, 2].imshow(img_np)\n",
    "    axes[idx, 2].imshow(pred_np, cmap='Greens', alpha=0.5)\n",
    "    axes[idx, 2].set_title(f'Prediction (Dice: {dice:.4f})')\n",
    "    axes[idx, 2].axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(MODELS_DIR / 'test_predictions.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e42d452",
   "metadata": {},
   "source": [
    "## 15. Download Model (Optional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f599f9dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download best model to local machine\n",
    "from google.colab import files\n",
    "\n",
    "print(\"Downloading best model...\")\n",
    "files.download(str(MODELS_DIR / 'best_model.pth'))\n",
    "\n",
    "print(\"Downloading training curves...\")\n",
    "files.download(str(MODELS_DIR / 'training_curves.png'))\n",
    "\n",
    "print(\"Downloading predictions...\")\n",
    "files.download(str(MODELS_DIR / 'test_predictions.png'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf42a19e",
   "metadata": {},
   "source": [
    "## ðŸŽ‰ Done!\n",
    "\n",
    "Your model has been trained successfully!\n",
    "\n",
    "### Results:\n",
    "- Best model saved to: `{MODELS_DIR}/best_model.pth`\n",
    "- Training curves: `{MODELS_DIR}/training_curves.png`\n",
    "- Test predictions: `{MODELS_DIR}/test_predictions.png`\n",
    "\n",
    "### Next Steps:\n",
    "1. Download the model files\n",
    "2. Use the model for inference on new images\n",
    "3. Fine-tune with different hyperparameters if needed"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
