"""
Configuration file for DWI Ischemic Stroke Segmentation Project
Contains all hyperparameters, paths, and settings
"""

import os
from pathlib import Path

# ==================== Project Paths ====================
PROJECT_ROOT = Path(__file__).parent
DATA_RAW = PROJECT_ROOT / "1_data_raw"
DATA_PROCESSED = PROJECT_ROOT / "2_data_processed"
MODEL_WEIGHTS = PROJECT_ROOT / "3_model_weights"
RESULTS_DIR = PROJECT_ROOT / "4_results"

# Raw data subdirectories
RAW_IMAGES_DIR = DATA_RAW / "images"
RAW_MASKS_DIR = DATA_RAW / "masks"

# Processed data subdirectories
PROCESSED_TRAIN_IMG = DATA_PROCESSED / "train" / "images"
PROCESSED_TRAIN_MASK = DATA_PROCESSED / "train" / "masks"
PROCESSED_VAL_IMG = DATA_PROCESSED / "val" / "images"
PROCESSED_VAL_MASK = DATA_PROCESSED / "val" / "masks"
PROCESSED_TEST_IMG = DATA_PROCESSED / "test" / "images"
PROCESSED_TEST_MASK = DATA_PROCESSED / "test" / "masks"

# Results subdirectories
PLOTS_DIR = RESULTS_DIR / "plots"
PREDICTIONS_DIR = RESULTS_DIR / "predictions"

# ==================== Data Parameters ====================
# Image specifications
IMAGE_SIZE = (512, 512)  # (Height, Width) - ‡∏à‡∏∞ resize ‡∏ó‡∏∏‡∏Å‡∏†‡∏≤‡∏û‡πÉ‡∏´‡πâ‡πÄ‡∏õ‡πá‡∏ô‡∏Ç‡∏ô‡∏≤‡∏î‡∏ô‡∏µ‡πâ
ORIGINAL_SIZE = None  # ‡∏à‡∏∞‡∏ñ‡∏π‡∏Å‡∏ï‡∏£‡∏ß‡∏à‡∏à‡∏±‡∏ö‡∏≠‡∏±‡∏ï‡πÇ‡∏ô‡∏°‡∏±‡∏ï‡∏¥‡∏à‡∏≤‡∏Å‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•

# Data split ratio
TRAIN_RATIO = 0.70
VAL_RATIO = 0.15
TEST_RATIO = 0.15

# Random seed for reproducibility
RANDOM_SEED = 42

# Minimum slices per patient (for filtering)
MIN_SLICES_PER_PATIENT = 1  # ‡∏ï‡∏±‡πâ‡∏á‡πÄ‡∏õ‡πá‡∏ô 1 ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏£‡∏ß‡∏°‡∏ó‡∏∏‡∏Å patient (‡πÉ‡∏ä‡πâ zero padding)

# File naming pattern
# Format: Patient_{XXX}_Slice_{YYY}.{extension}
PATIENT_PATTERN = r'Patient_(\d+)_Slice_(\d+)'  # Regex pattern for parsing filenames

# ==================== Preprocessing Parameters ====================
# CLAHE (Contrast Limited Adaptive Histogram Equalization)
CLAHE_ENABLED = False
CLAHE_CLIP_LIMIT = 0.03  # ‡∏à‡∏≥‡∏Å‡∏±‡∏î‡∏Å‡∏≤‡∏£‡πÄ‡∏û‡∏¥‡πà‡∏° contrast (‡∏Ñ‡πà‡∏≤‡∏ï‡πà‡∏≥ = ‡∏≠‡πà‡∏≠‡∏ô‡πÇ‡∏¢‡∏ô, ‡∏Ñ‡πà‡∏≤‡∏™‡∏π‡∏á = ‡πÅ‡∏£‡∏á)
CLAHE_KERNEL_SIZE = None  # None = auto-calculate based on image size

# Normalization
NORMALIZE_METHOD = 'zscore'  # 'zscore', 'minmax', or 'none'
# Z-score parameters (‡∏à‡∏∞‡∏Ñ‡∏≥‡∏ô‡∏ß‡∏ì‡∏à‡∏≤‡∏Å training set)
TRAIN_MEAN = None  # ‡∏à‡∏∞‡∏ñ‡∏π‡∏Å‡∏Ñ‡∏≥‡∏ô‡∏ß‡∏ì‡πÅ‡∏•‡∏∞‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å‡πÉ‡∏ô preprocessing
TRAIN_STD = None   # ‡∏à‡∏∞‡∏ñ‡∏π‡∏Å‡∏Ñ‡∏≥‡∏ô‡∏ß‡∏ì‡πÅ‡∏•‡∏∞‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å‡πÉ‡∏ô preprocessing

# ==================== Model Architecture Parameters ====================
# Input
IN_CHANNELS = 3  # 2.5D input: [N-1, N, N+1] slices

# U-Net architecture
ENCODER_CHANNELS = [64, 128, 256, 512]  # Channels ‡πÉ‡∏ô‡πÅ‡∏ï‡πà‡∏•‡∏∞ layer ‡∏Ç‡∏≠‡∏á encoder
DECODER_CHANNELS = [512, 256, 128, 64]  # Channels ‡πÉ‡∏ô‡πÅ‡∏ï‡πà‡∏•‡∏∞ layer ‡∏Ç‡∏≠‡∏á decoder
BOTTLENECK_CHANNELS = 1024  # Channels ‡∏ó‡∏µ‡πà‡∏à‡∏∏‡∏î‡∏Å‡∏∂‡πà‡∏á‡∏Å‡∏•‡∏≤‡∏á (‡∏•‡∏∂‡∏Å‡∏ó‡∏µ‡πà‡∏™‡∏∏‡∏î)

# Output
OUT_CHANNELS = 1  # Binary segmentation (background vs lesion)

# Attention Gate
USE_ATTENTION = True  # ‡πÄ‡∏õ‡∏¥‡∏î/‡∏õ‡∏¥‡∏î Attention Gates

# ==================== Training Parameters ====================
# Basic training settings
NUM_EPOCHS = 200
BATCH_SIZE = 16  # ‡∏õ‡∏£‡∏±‡∏ö‡∏ï‡∏≤‡∏° GPU memory (‡∏ñ‡πâ‡∏≤ out of memory ‡πÉ‡∏´‡πâ‡∏•‡∏î‡∏•‡∏á)
NUM_WORKERS = 4  # ‡∏à‡∏≥‡∏ô‡∏ß‡∏ô workers ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö DataLoader

# Optimizer
OPTIMIZER = 'adamw'  # 'adam' or 'adamw'
LEARNING_RATE = 3e-5  # ‚¨áÔ∏è ‡∏•‡∏î‡∏•‡∏á‡∏à‡∏≤‡∏Å 1e-4 ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏õ‡πâ‡∏≠‡∏á‡∏Å‡∏±‡∏ô gradient explosion
WEIGHT_DECAY = 1e-5  # L2 regularization

# Gradient clipping (‡∏õ‡πâ‡∏≠‡∏á‡∏Å‡∏±‡∏ô exploding gradients)
GRADIENT_CLIP_VALUE = 1.0  # Clip gradients ‡∏ó‡∏µ‡πà‡∏°‡∏µ‡∏Ñ‡πà‡∏≤‡∏°‡∏≤‡∏Å‡∏Å‡∏ß‡πà‡∏≤ 1.0

# Learning rate scheduler
SCHEDULER = 'reduce_on_plateau'  # 'reduce_on_plateau' or 'cosine'
SCHEDULER_PATIENCE = 5  # ‡∏à‡∏≥‡∏ô‡∏ß‡∏ô epochs ‡∏ó‡∏µ‡πà‡∏£‡∏≠‡∏Å‡πà‡∏≠‡∏ô‡∏•‡∏î LR
SCHEDULER_FACTOR = 0.5  # ‡∏•‡∏î LR ‡πÄ‡∏õ‡πá‡∏ô 0.5 ‡πÄ‡∏ó‡πà‡∏≤
SCHEDULER_MIN_LR = 1e-7  # LR ‡∏ï‡πà‡∏≥‡∏™‡∏∏‡∏î

# Loss function
LOSS_TYPE = 'combo'  # 'focal', 'dice', or 'combo'
FOCAL_ALPHA = 0.25  # Weight for positive class in Focal Loss
FOCAL_GAMMA = 2.0   # Focusing parameter (‡∏¢‡∏¥‡πà‡∏á‡∏™‡∏π‡∏á ‡∏¢‡∏¥‡πà‡∏á‡πÇ‡∏ü‡∏Å‡∏±‡∏™‡∏ó‡∏µ‡πà hard examples)
DICE_SMOOTH = 1e-6  # Smoothing factor for Dice Loss
COMBO_FOCAL_WEIGHT = 0.3  # ‚¨áÔ∏è ‡∏•‡∏î‡∏ô‡πâ‡∏≥‡∏´‡∏ô‡∏±‡∏Å Focal Loss (‡∏°‡∏±‡∏Å‡∏ó‡∏≥‡πÉ‡∏´‡πâ‡πÑ‡∏°‡πà‡πÄ‡∏™‡∏ñ‡∏µ‡∏¢‡∏£)
COMBO_DICE_WEIGHT = 0.7   # ‚¨ÜÔ∏è ‡πÄ‡∏û‡∏¥‡πà‡∏°‡∏ô‡πâ‡∏≥‡∏´‡∏ô‡∏±‡∏Å Dice Loss (‡πÄ‡∏™‡∏ñ‡∏µ‡∏¢‡∏£‡∏Å‡∏ß‡πà‡∏≤)

# Early stopping
EARLY_STOPPING_PATIENCE = 15  # ‡∏´‡∏¢‡∏∏‡∏î‡∏ñ‡πâ‡∏≤ val dice ‡πÑ‡∏°‡πà‡∏î‡∏µ‡∏Ç‡∏∂‡πâ‡∏ô‡πÄ‡∏õ‡πá‡∏ô‡πÄ‡∏ß‡∏•‡∏≤ 15 epochs
EARLY_STOPPING_MIN_DELTA = 1e-4  # ‡∏Å‡∏≤‡∏£‡πÄ‡∏õ‡∏•‡∏µ‡πà‡∏¢‡∏ô‡πÅ‡∏õ‡∏•‡∏á‡∏Ç‡∏±‡πâ‡∏ô‡∏ï‡πà‡∏≥‡∏ó‡∏µ‡πà‡∏ñ‡∏∑‡∏≠‡∏ß‡πà‡∏≤ "‡∏î‡∏µ‡∏Ç‡∏∂‡πâ‡∏ô"

# Checkpointing
SAVE_BEST_ONLY = True  # ‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å‡πÄ‡∏â‡∏û‡∏≤‡∏∞‡πÇ‡∏°‡πÄ‡∏î‡∏•‡∏ó‡∏µ‡πà‡∏î‡∏µ‡∏ó‡∏µ‡πà‡∏™‡∏∏‡∏î
CHECKPOINT_METRIC = 'val_dice'  # Metric ‡∏ó‡∏µ‡πà‡πÉ‡∏ä‡πâ‡πÉ‡∏ô‡∏Å‡∏≤‡∏£‡∏ï‡∏±‡∏î‡∏™‡∏¥‡∏ô‡πÉ‡∏à
CHECKPOINT_MODE = 'max'  # 'max' (‡∏™‡∏π‡∏á‡∏Å‡∏ß‡πà‡∏≤ = ‡∏î‡∏µ‡∏Å‡∏ß‡πà‡∏≤) or 'min' (‡∏ï‡πà‡∏≥‡∏Å‡∏ß‡πà‡∏≤ = ‡∏î‡∏µ‡∏Å‡∏ß‡πà‡∏≤)

# ==================== Data Augmentation Parameters ====================
AUGMENTATION_ENABLED = False

# Augmentation probabilities (0.0 = ‡πÑ‡∏°‡πà‡πÉ‡∏ä‡πâ, 1.0 = ‡πÉ‡∏ä‡πâ‡∏ó‡∏∏‡∏Å‡∏Ñ‡∏£‡∏±‡πâ‡∏á)
AUG_HORIZONTAL_FLIP_PROB = 0.5
AUG_VERTICAL_FLIP_PROB = 0.0  # ‡πÑ‡∏°‡πà‡πÅ‡∏ô‡∏∞‡∏ô‡∏≥‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö medical images
AUG_ROTATE_PROB = 0.3
AUG_ROTATE_LIMIT = 15  # ‡∏´‡∏°‡∏∏‡∏ô‡πÑ‡∏î‡πâ‡∏™‡∏π‡∏á‡∏™‡∏∏‡∏î ¬±15 ‡∏≠‡∏á‡∏®‡∏≤

AUG_ELASTIC_TRANSFORM_PROB = 0.4  # ‡∏™‡∏≥‡∏Ñ‡∏±‡∏ç‡∏°‡∏≤‡∏Å! ‡∏à‡∏≥‡∏•‡∏≠‡∏á‡∏Å‡∏≤‡∏£‡∏ö‡∏¥‡∏î‡πÄ‡∏ö‡∏µ‡πâ‡∏¢‡∏ß‡∏Ç‡∏≠‡∏á‡πÄ‡∏ô‡∏∑‡πâ‡∏≠‡πÄ‡∏¢‡∏∑‡πà‡∏≠
AUG_ELASTIC_ALPHA = 1.0
AUG_ELASTIC_SIGMA = 50.0

AUG_BRIGHTNESS_CONTRAST_PROB = 0.3
AUG_BRIGHTNESS_LIMIT = 0.1
AUG_CONTRAST_LIMIT = 0.1

AUG_GAUSSIAN_NOISE_PROB = 0.2
AUG_GAUSSIAN_NOISE_VAR = (10.0, 50.0)

# ==================== Evaluation Parameters ====================
# Metrics
EVAL_METRICS = ['dice', 'iou', 'precision', 'recall', 'f1']

# Visualization
NUM_QUALITATIVE_SAMPLES = 10  # ‡∏à‡∏≥‡∏ô‡∏ß‡∏ô‡∏ï‡∏±‡∏ß‡∏≠‡∏¢‡πà‡∏≤‡∏á‡∏ó‡∏µ‡πà‡∏à‡∏∞‡πÅ‡∏™‡∏î‡∏á‡πÉ‡∏ô evaluation
VIZ_ALPHA = 0.5  # ‡∏Ñ‡∏ß‡∏≤‡∏°‡πÇ‡∏õ‡∏£‡πà‡∏á‡πÉ‡∏™‡∏Ç‡∏≠‡∏á mask overlay (0.0-1.0)
VIZ_GT_COLOR = (1.0, 0.0, 0.0)  # ‡∏™‡∏µ‡πÅ‡∏î‡∏á‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö Ground Truth
VIZ_PRED_COLOR = (0.0, 1.0, 1.0)  # ‡∏™‡∏µ‡∏ü‡πâ‡∏≤‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö Prediction

# Threshold for binary mask
PREDICTION_THRESHOLD = 0.5  # Threshold ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡πÅ‡∏õ‡∏•‡∏á probability ‡πÄ‡∏õ‡πá‡∏ô binary mask

# ==================== Hardware Settings ====================
import torch

# Device configuration
USE_CUDA = torch.cuda.is_available()
DEVICE = torch.device('cuda' if USE_CUDA else 'cpu')
NUM_GPUS = torch.cuda.device_count() if USE_CUDA else 0

# Mixed precision training (faster training on modern GPUs)
USE_MIXED_PRECISION = True if USE_CUDA else False

# ==================== Logging Settings ====================
LOG_INTERVAL = 10  # ‡∏û‡∏¥‡∏°‡∏û‡πå progress ‡∏ó‡∏∏‡∏Å‡πÜ N batches
SAVE_LOG_FILE = True
LOG_FILE = RESULTS_DIR / "training_log.txt"

# Tensorboard
USE_TENSORBOARD = False  # ‡πÄ‡∏õ‡∏¥‡∏î/‡∏õ‡∏¥‡∏î Tensorboard logging
TENSORBOARD_DIR = RESULTS_DIR / "tensorboard"

# ==================== Helper Functions ====================
def create_directories():
    """‡∏™‡∏£‡πâ‡∏≤‡∏á directories ‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î‡∏ó‡∏µ‡πà‡∏à‡∏≥‡πÄ‡∏õ‡πá‡∏ô"""
    dirs = [
        DATA_RAW, DATA_PROCESSED, MODEL_WEIGHTS, RESULTS_DIR,
        RAW_IMAGES_DIR, RAW_MASKS_DIR,
        PROCESSED_TRAIN_IMG, PROCESSED_TRAIN_MASK,
        PROCESSED_VAL_IMG, PROCESSED_VAL_MASK,
        PROCESSED_TEST_IMG, PROCESSED_TEST_MASK,
        PLOTS_DIR, PREDICTIONS_DIR
    ]
    
    if USE_TENSORBOARD:
        dirs.append(TENSORBOARD_DIR)
    
    for directory in dirs:
        directory.mkdir(parents=True, exist_ok=True)
    
    print("‚úÖ Created all necessary directories")


def print_config():
    """‡∏û‡∏¥‡∏°‡∏û‡πå configuration ‡∏™‡∏≥‡∏Ñ‡∏±‡∏ç‡πÜ"""
    print("\n" + "="*60)
    print("üîß DWI ISCHEMIC STROKE SEGMENTATION - CONFIGURATION")
    print("="*60)
    print(f"\nüìÅ Paths:")
    print(f"   Project Root: {PROJECT_ROOT}")
    print(f"   Raw Data: {DATA_RAW}")
    print(f"   Processed Data: {DATA_PROCESSED}")
    print(f"   Model Weights: {MODEL_WEIGHTS}")
    print(f"   Results: {RESULTS_DIR}")
    
    print(f"\nüìä Data:")
    print(f"   Image Size: {IMAGE_SIZE}")
    print(f"   Train/Val/Test Split: {TRAIN_RATIO}/{VAL_RATIO}/{TEST_RATIO}")
    print(f"   Min Slices per Patient: {MIN_SLICES_PER_PATIENT}")
    
    print(f"\nüî¨ Preprocessing:")
    print(f"   CLAHE Enabled: {CLAHE_ENABLED}")
    print(f"   CLAHE Clip Limit: {CLAHE_CLIP_LIMIT}")
    print(f"   Normalization: {NORMALIZE_METHOD}")
    
    print(f"\nüèóÔ∏è Model:")
    print(f"   Architecture: Attention U-Net (2.5D)")
    print(f"   Input Channels: {IN_CHANNELS}")
    print(f"   Encoder Channels: {ENCODER_CHANNELS}")
    print(f"   Bottleneck: {BOTTLENECK_CHANNELS}")
    print(f"   Use Attention: {USE_ATTENTION}")
    
    print(f"\nüéì Training:")
    print(f"   Epochs: {NUM_EPOCHS}")
    print(f"   Batch Size: {BATCH_SIZE}")
    print(f"   Learning Rate: {LEARNING_RATE}")
    print(f"   Optimizer: {OPTIMIZER.upper()}")
    print(f"   Loss: {LOSS_TYPE.upper()}")
    if LOSS_TYPE == 'combo':
        print(f"      Focal Weight: {COMBO_FOCAL_WEIGHT}, Dice Weight: {COMBO_DICE_WEIGHT}")
    print(f"   Scheduler: {SCHEDULER}")
    print(f"   Early Stopping Patience: {EARLY_STOPPING_PATIENCE}")
    
    print(f"\nüñºÔ∏è Augmentation:")
    print(f"   Enabled: {AUGMENTATION_ENABLED}")
    if AUGMENTATION_ENABLED:
        print(f"   Horizontal Flip: {AUG_HORIZONTAL_FLIP_PROB}")
        print(f"   Rotation: {AUG_ROTATE_PROB} (¬±{AUG_ROTATE_LIMIT}¬∞)")
        print(f"   Elastic Transform: {AUG_ELASTIC_TRANSFORM_PROB}")
        print(f"   Brightness/Contrast: {AUG_BRIGHTNESS_CONTRAST_PROB}")
    
    print(f"\nüíª Hardware:")
    print(f"   Device: {DEVICE}")
    if USE_CUDA:
        print(f"   GPU(s): {NUM_GPUS} x {torch.cuda.get_device_name(0)}")
        print(f"   Mixed Precision: {USE_MIXED_PRECISION}")
    
    print("\n" + "="*60 + "\n")


def get_model_save_path(name='best_model'):
    """‡∏™‡∏£‡πâ‡∏≤‡∏á path ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å‡πÇ‡∏°‡πÄ‡∏î‡∏•"""
    return MODEL_WEIGHTS / f"{name}.pth"


def get_checkpoint_path(epoch):
    """‡∏™‡∏£‡πâ‡∏≤‡∏á path ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö checkpoint ‡πÅ‡∏ï‡πà‡∏•‡∏∞ epoch"""
    return MODEL_WEIGHTS / f"checkpoint_epoch_{epoch:03d}.pth"


if __name__ == "__main__":
    # Test configuration
    print_config()
    create_directories()
    print("‚úÖ Configuration loaded successfully!")
