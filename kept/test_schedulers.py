"""
Test script for advanced learning rate schedulers
Tests all 6 scheduler types to verify proper integration
"""

import sys
import torch
import torch.nn as nn
import torch.optim as optim
import matplotlib.pyplot as plt
import numpy as np

# Import schedulers
from lr_schedulers import get_advanced_scheduler


class DummyModel(nn.Module):
    """Simple model for testing"""
    def __init__(self):
        super().__init__()
        self.conv = nn.Conv2d(3, 64, 3, padding=1)
    
    def forward(self, x):
        return self.conv(x)


class DummyConfig:
    """Mock config for testing"""
    def __init__(self):
        self.LEARNING_RATE = 0.001
        self.NUM_EPOCHS = 100
        self.WARMUP_EPOCHS = 5
        self.FIRST_CYCLE_EPOCHS = 30
        self.CYCLE_MULT = 2
        self.POLY_POWER = 2.0
        self.EXP_GAMMA = 0.95
        self.SCHEDULER_MIN_LR = 1e-7
        self.SCHEDULER_PATIENCE = 10
        self.SCHEDULER_FACTOR = 0.5
        self._train_loader_size = 100  # For OneCycleLR


def test_scheduler(scheduler_type, epochs=100):
    """
    Test a scheduler and return LR trajectory
    
    Args:
        scheduler_type: Type of scheduler to test
        epochs: Number of epochs to simulate
    
    Returns:
        list: Learning rates over epochs
    """
    print(f"\n{'='*70}")
    print(f"Testing: {scheduler_type.upper()}")
    print(f"{'='*70}")
    
    # Create model and optimizer
    model = DummyModel()
    optimizer = optim.Adam(model.parameters(), lr=0.001)
    cfg = DummyConfig()
    
    # Create scheduler
    try:
        result, metadata = get_advanced_scheduler(optimizer, scheduler_type, cfg)
        
        print(f"âœ“ Scheduler created successfully")
        print(f"  Description: {metadata['description']}")
        print(f"  Steps per batch: {metadata.get('step_per_batch', False)}")
        print(f"  Requires metric: {metadata.get('requires_metric', False)}")
        
        # Handle warmup_cosine dict return
        if isinstance(result, dict):
            warmup_scheduler = result['warmup']
            main_scheduler = result['main']
            print(f"  Has warmup: Yes ({cfg.WARMUP_EPOCHS} epochs)")
            
            # Collect LRs
            lrs = []
            for epoch in range(epochs):
                if epoch < cfg.WARMUP_EPOCHS:
                    warmup_scheduler.step(epoch)
                else:
                    main_scheduler.step()
                lrs.append(optimizer.param_groups[0]['lr'])
        else:
            scheduler = result
            print(f"  Has warmup: No")
            
            # Collect LRs
            lrs = []
            for epoch in range(epochs):
                if metadata.get('requires_metric', False):
                    # Simulate validation metric (decreasing over time with noise)
                    val_metric = 0.5 + 0.3 * (1 - epoch / epochs) + np.random.randn() * 0.02
                    scheduler.step(val_metric)
                else:
                    scheduler.step()
                lrs.append(optimizer.param_groups[0]['lr'])
        
        print(f"  Initial LR: {lrs[0]:.6f}")
        print(f"  Final LR: {lrs[-1]:.6f}")
        print(f"  Min LR: {min(lrs):.6f}")
        print(f"  Max LR: {max(lrs):.6f}")
        
        return lrs, metadata
        
    except Exception as e:
        print(f"âœ— Error: {e}")
        import traceback
        traceback.print_exc()
        return None, None


def plot_schedulers():
    """
    Plot all schedulers on one graph for comparison
    """
    schedulers_to_test = [
        'warmup_cosine',
        'cosine_restarts', 
        'onecycle',
        'polynomial',
        'adaptive',
        'exponential'
    ]
    
    fig, axes = plt.subplots(2, 3, figsize=(15, 10))
    fig.suptitle('Advanced Learning Rate Schedulers Comparison', fontsize=16, fontweight='bold')
    
    axes = axes.flatten()
    
    for idx, scheduler_type in enumerate(schedulers_to_test):
        lrs, metadata = test_scheduler(scheduler_type)
        
        if lrs is not None:
            ax = axes[idx]
            ax.plot(lrs, linewidth=2, color='#2E86AB')
            ax.set_title(f"{scheduler_type.upper()}\n{metadata['description']}", 
                        fontsize=10, fontweight='bold')
            ax.set_xlabel('Epoch')
            ax.set_ylabel('Learning Rate')
            ax.grid(True, alpha=0.3)
            ax.set_yscale('log')
            
            # Add annotations
            ax.axhline(y=lrs[0], color='green', linestyle='--', alpha=0.5, label='Initial LR')
            ax.axhline(y=lrs[-1], color='red', linestyle='--', alpha=0.5, label='Final LR')
            ax.legend(fontsize=8)
    
    plt.tight_layout()
    plt.savefig('scheduler_comparison.png', dpi=150, bbox_inches='tight')
    print(f"\n{'='*70}")
    print(f"ðŸ“Š Plot saved to: scheduler_comparison.png")
    print(f"{'='*70}")
    plt.show()


def test_warmup_stability():
    """
    Test that warmup prevents large initial updates
    """
    print(f"\n{'='*70}")
    print(f"WARMUP STABILITY TEST")
    print(f"{'='*70}")
    
    model = DummyModel()
    cfg = DummyConfig()
    
    # Test WITH warmup
    print("\n1. WITH Warmup (warmup_cosine):")
    optimizer_warmup = optim.Adam(model.parameters(), lr=0.001)
    result, _ = get_advanced_scheduler(optimizer_warmup, 'warmup_cosine', cfg)
    warmup_scheduler = result['warmup']
    
    lrs_with_warmup = []
    for epoch in range(10):
        warmup_scheduler.step(epoch)
        lrs_with_warmup.append(optimizer_warmup.param_groups[0]['lr'])
    
    print(f"   Epoch 0: {lrs_with_warmup[0]:.6f}")
    print(f"   Epoch 5: {lrs_with_warmup[5]:.6f}")
    print(f"   Epoch 9: {lrs_with_warmup[9]:.6f}")
    print(f"   âœ“ LR gradually increases: {lrs_with_warmup[0]:.6f} â†’ {lrs_with_warmup[9]:.6f}")
    
    # Test WITHOUT warmup
    print("\n2. WITHOUT Warmup (standard start):")
    optimizer_no_warmup = optim.Adam(model.parameters(), lr=0.001)
    print(f"   Epoch 0: {optimizer_no_warmup.param_groups[0]['lr']:.6f}")
    print(f"   âš ï¸  Starts at full LR immediately")
    
    print(f"\nðŸ’¡ Warmup prevents large gradient updates when model is uninitialized!")
    print(f"   This is critical for attention mechanisms which amplify gradients.")


def main():
    """Run all scheduler tests"""
    print("="*70)
    print("ADVANCED LEARNING RATE SCHEDULER TEST SUITE")
    print("="*70)
    
    # Test each scheduler
    plot_schedulers()
    
    # Test warmup stability
    test_warmup_stability()
    
    print(f"\n{'='*70}")
    print("âœ“ All scheduler tests completed successfully!")
    print(f"{'='*70}")
    
    print("\nðŸ“‹ SUMMARY:")
    print("   â€¢ All 6 schedulers initialized correctly")
    print("   â€¢ Warmup prevents early instability")
    print("   â€¢ Different strategies for different use cases")
    print("   â€¢ Ready for production training")
    
    print("\nðŸŽ¯ NEXT STEPS:")
    print("   1. Run: python train.py")
    print("   2. Monitor LR in logs and MLflow")
    print("   3. Watch for NaN prevention during warmup")
    print("   4. Compare validation metrics across schedulers")


if __name__ == '__main__':
    main()
