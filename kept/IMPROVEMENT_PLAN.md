# üöÄ DWI Segmentation Model Improvement Plan
## ‡∏Å‡∏≤‡∏£‡∏õ‡∏£‡∏±‡∏ö‡∏õ‡∏£‡∏∏‡∏á‡πÅ‡∏ö‡∏ö‡∏Ñ‡∏£‡∏ö‡∏ß‡∏á‡∏à‡∏£ (Comprehensive Upgrade Plan)

**‡∏ß‡∏±‡∏ô‡∏ó‡∏µ‡πà:** 27 ‡∏û‡∏§‡∏®‡∏à‡∏¥‡∏Å‡∏≤‡∏¢‡∏ô 2568  
**‡∏™‡∏ñ‡∏≤‡∏ô‡∏∞:** ‚úÖ Phase 1-2 ‡πÄ‡∏™‡∏£‡πá‡∏à‡∏™‡∏¥‡πâ‡∏ô | üîÑ Phase 3-4 ‡∏û‡∏£‡πâ‡∏≠‡∏°‡∏î‡∏≥‡πÄ‡∏ô‡∏¥‡∏ô‡∏Å‡∏≤‡∏£

---

## üìä **‡∏™‡∏£‡∏∏‡∏õ‡∏Å‡∏≤‡∏£‡∏õ‡∏£‡∏±‡∏ö‡∏õ‡∏£‡∏∏‡∏á‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î (4 Phases)**

| Phase | Feature | Status | Expected Gain | Risk | Time |
|-------|---------|--------|--------------|------|------|
| 1 | TTA + CCA | ‚úÖ **‡πÄ‡∏™‡∏£‡πá‡∏à‡πÅ‡∏•‡πâ‡∏ß** | +2-4% Dice | ‡∏ï‡πà‡∏≥ | 2h |
| 2 | N4 Bias Correction | ‚úÖ **‡πÄ‡∏™‡∏£‡πá‡∏à‡πÅ‡∏•‡πâ‡∏ß** | +3-6% Dice | ‡∏ï‡πà‡∏≥ | 4h |
| 3 | Gamma + Log-Cosh Loss | üîÑ ‡∏û‡∏£‡πâ‡∏≠‡∏°‡∏ó‡∏≥ | +2-3% Dice | ‡∏Å‡∏•‡∏≤‡∏á | 3h |
| 4 | Deep Supervision | üîÑ ‡∏û‡∏£‡πâ‡∏≠‡∏°‡∏ó‡∏≥ | +2-4% Dice | ‡∏Å‡∏•‡∏≤‡∏á | 6h |

**‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå‡∏£‡∏ß‡∏°‡∏ó‡∏µ‡πà‡∏Ñ‡∏≤‡∏î‡∏´‡∏ß‡∏±‡∏á:**
- Conservative: Test Dice 62% ‚Üí 73% (+11%)
- Best Case: Test Dice 62% ‚Üí 79% (+17%)

---

## ‚úÖ **PHASE 1: Test-Time Augmentation + CCA Cleaning**

### ‡∏™‡∏ñ‡∏≤‡∏ô‡∏∞: ‚úÖ ‡πÄ‡∏™‡∏£‡πá‡∏à‡∏™‡∏°‡∏ö‡∏π‡∏£‡∏ì‡πå

### ‡πÑ‡∏ü‡∏•‡πå‡∏ó‡∏µ‡πà‡πÅ‡∏Å‡πâ‡πÑ‡∏Ç:
1. ‚úÖ `evaluation_module.py` - ‡πÄ‡∏û‡∏¥‡πà‡∏° `TTAWrapper`, `apply_cca_cleaning`, `run_evaluation_with_tta`
2. ‚úÖ `evaluate.py` - ‡∏ú‡∏™‡∏≤‡∏ô TTA+CCA ‡πÄ‡∏Ç‡πâ‡∏≤‡∏Å‡∏±‡∏ö evaluation pipeline
3. ‚úÖ `config.py` - ‡πÄ‡∏û‡∏¥‡πà‡∏° parameters:
   ```python
   USE_TTA = True
   TTA_AUGMENTATIONS = ['hflip', 'vflip']
   USE_CCA = True
   CCA_MIN_SIZE = 10
   CCA_MIN_CONFIDENCE = 0.3
   ```

### ‡∏Ñ‡∏∏‡∏ì‡∏™‡∏°‡∏ö‡∏±‡∏ï‡∏¥‡πÉ‡∏´‡∏°‡πà:

#### 1. Test-Time Augmentation (TTA)
- ‡∏£‡∏≠‡∏á‡∏£‡∏±‡∏ö: `hflip`, `vflip`, `rot90`, `rot180`, `rot270`
- Average predictions ‡∏à‡∏≤‡∏Å‡∏´‡∏•‡∏≤‡∏¢ augmentations
- ‡πÑ‡∏°‡πà‡∏ï‡πâ‡∏≠‡∏á retrain model
- Inference ‡∏ä‡πâ‡∏≤‡∏Ç‡∏∂‡πâ‡∏ô 2-5x (‡∏Ç‡∏∂‡πâ‡∏ô‡∏Å‡∏±‡∏ö‡∏à‡∏≥‡∏ô‡∏ß‡∏ô augmentations)

#### 2. Connected Component Analysis (CCA)
- ‡∏Å‡∏£‡∏≠‡∏á components ‡∏ï‡∏≤‡∏° size (pixels)
- ‡∏Å‡∏£‡∏≠‡∏á components ‡∏ï‡∏≤‡∏° confidence (probability)
- ‡∏•‡∏î false positives ‡πÑ‡∏î‡πâ 40-60%

### ‡∏Å‡∏≤‡∏£‡πÉ‡∏ä‡πâ‡∏á‡∏≤‡∏ô:
```bash
# Run evaluation with TTA + CCA
python evaluate.py

# ‡∏õ‡∏¥‡∏î TTA/CCA (‡πÅ‡∏Å‡πâ‡πÉ‡∏ô config.py)
USE_TTA = False
USE_CCA = False
```

### ‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå‡∏ó‡∏µ‡πà‡∏Ñ‡∏≤‡∏î‡∏´‡∏ß‡∏±‡∏á:
- **Test Dice: +2-4%**
- **Precision: +5-8%**
- **Prediction variance: ‡∏•‡∏î‡∏•‡∏á**
- **False positives: ‡∏•‡∏î‡∏•‡∏á 40-60%**

---

## ‚úÖ **PHASE 2: N4 Bias Field Correction**

### ‡∏™‡∏ñ‡∏≤‡∏ô‡∏∞: ‚úÖ ‡πÄ‡∏™‡∏£‡πá‡∏à‡∏™‡∏°‡∏ö‡∏π‡∏£‡∏ì‡πå

### ‡πÑ‡∏ü‡∏•‡πå‡∏ó‡∏µ‡πà‡πÅ‡∏Å‡πâ‡πÑ‡∏Ç:
1. ‚úÖ `01_preprocess.py` - ‡πÄ‡∏û‡∏¥‡πà‡∏°:
   - `apply_n4_bias_correction()` - N4 correction function
   - `apply_n4_parallel()` - Parallel processing
   - ‡πÅ‡∏Å‡πâ‡πÑ‡∏Ç `process_and_save()` - ‡∏£‡∏≠‡∏á‡∏£‡∏±‡∏ö N4
   
2. ‚úÖ `config.py` - ‡πÄ‡∏û‡∏¥‡πà‡∏° parameters:
   ```python
   N4_ENABLED = True
   N4_SHRINK_FACTOR = 4  # Speed vs quality
   N4_NUM_ITERATIONS = 50  # Correction iterations
   N4_NUM_WORKERS = 4  # Parallel workers
   ```

### ‡∏Ñ‡∏∏‡∏ì‡∏™‡∏°‡∏ö‡∏±‡∏ï‡∏¥‡πÉ‡∏´‡∏°‡πà:

#### N4 Bias Field Correction
- ‡πÅ‡∏Å‡πâ‡∏õ‡∏±‡∏ç‡∏´‡∏≤ intensity inhomogeneity ‡πÉ‡∏ô MRI
- ‡∏ó‡∏≥‡πÉ‡∏´‡πâ brightness ‡∏™‡∏°‡πà‡∏≥‡πÄ‡∏™‡∏°‡∏≠‡∏ó‡∏±‡πà‡∏ß‡∏ó‡∏±‡πâ‡∏á‡∏†‡∏≤‡∏û
- ‡πÄ‡∏û‡∏¥‡πà‡∏° lesion visibility
- Multiprocessing ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏Ñ‡∏ß‡∏≤‡∏°‡πÄ‡∏£‡πá‡∏ß

### Pipeline ‡πÉ‡∏´‡∏°‡πà:
```
1. Load image
2. ‚ú® Apply N4 Bias Correction (NEW)
3. Resize to target size
4. Apply CLAHE (optional)
5. Normalize (Z-score)
6. Save as .npy
```

### Requirements:
```bash
pip install SimpleITK
```

### ‡∏Å‡∏≤‡∏£‡πÉ‡∏ä‡πâ‡∏á‡∏≤‡∏ô:
```bash
# Re-preprocess ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏î‡πâ‡∏ß‡∏¢ N4 correction
python 01_preprocess.py

# ‡∏à‡∏∞‡πÉ‡∏ä‡πâ‡πÄ‡∏ß‡∏•‡∏≤‡∏õ‡∏£‡∏∞‡∏°‡∏≤‡∏ì:
# - Single-threaded: ~2-4 ‡∏ä‡∏±‡πà‡∏ß‡πÇ‡∏°‡∏á (848 images)
# - Multi-threaded (4 workers): ~30-60 ‡∏ô‡∏≤‡∏ó‡∏µ
```

### ‡∏ï‡∏±‡∏ß‡πÄ‡∏•‡∏∑‡∏≠‡∏Å Configuration:
```python
# ‡πÄ‡∏£‡πá‡∏ß‡∏°‡∏≤‡∏Å ‡πÅ‡∏ï‡πà‡∏Ñ‡∏∏‡∏ì‡∏†‡∏≤‡∏û‡∏ï‡πà‡∏≥‡∏Å‡∏ß‡πà‡∏≤
N4_SHRINK_FACTOR = 8
N4_NUM_ITERATIONS = 25

# ‡∏™‡∏°‡∏î‡∏∏‡∏• (‡πÅ‡∏ô‡∏∞‡∏ô‡∏≥)
N4_SHRINK_FACTOR = 4
N4_NUM_ITERATIONS = 50

# ‡∏Ñ‡∏∏‡∏ì‡∏†‡∏≤‡∏û‡∏™‡∏π‡∏á‡∏™‡∏∏‡∏î ‡πÅ‡∏ï‡πà‡∏ä‡πâ‡∏≤
N4_SHRINK_FACTOR = 1
N4_NUM_ITERATIONS = 100
```

### ‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå‡∏ó‡∏µ‡πà‡∏Ñ‡∏≤‡∏î‡∏´‡∏ß‡∏±‡∏á:
- **Val Dice: +3-6%**
- **Test Dice: +3-6%**
- **‡∏†‡∏≤‡∏û‡∏°‡∏µ consistency ‡∏°‡∏≤‡∏Å‡∏Ç‡∏∂‡πâ‡∏ô**
- **Lesion boundaries ‡∏ä‡∏±‡∏î‡πÄ‡∏à‡∏ô‡∏Ç‡∏∂‡πâ‡∏ô**

---

## üîÑ **PHASE 3: Gamma Correction + Log-Cosh Dice Loss**

### ‡∏™‡∏ñ‡∏≤‡∏ô‡∏∞: üîÑ ‡∏û‡∏£‡πâ‡∏≠‡∏°‡∏î‡∏≥‡πÄ‡∏ô‡∏¥‡∏ô‡∏Å‡∏≤‡∏£

### ‡πÅ‡∏ú‡∏ô‡∏Å‡∏≤‡∏£:

#### 3A. Gamma Correction Augmentation
**‡πÑ‡∏ü‡∏•‡πå:** `dataset.py`

```python
class RandomGamma(A.ImageOnlyTransform):
    """
    Random Gamma Correction
    Simulates different MRI scanner settings
    """
    def __init__(self, gamma_limit=(0.7, 1.5), p=0.5):
        super().__init__(p=p)
        self.gamma_limit = gamma_limit
    
    def apply(self, img, gamma=1.0, **params):
        img_min, img_max = img.min(), img.max()
        img_norm = (img - img_min) / (img_max - img_min + 1e-8)
        img_corrected = np.power(img_norm, gamma)
        return img_corrected * (img_max - img_min) + img_min
```

**‡∏Å‡∏≤‡∏£‡πÉ‡∏ä‡πâ‡∏á‡∏≤‡∏ô:**
```python
# ‡πÄ‡∏û‡∏¥‡πà‡∏°‡πÉ‡∏ô get_training_augmentation()
transforms.append(RandomGamma(gamma_limit=(0.7, 1.5), p=0.5))
```

#### 3B. Log-Cosh Dice Loss
**‡πÑ‡∏ü‡∏•‡πå:** `loss.py`

```python
class LogCoshDiceLoss(nn.Module):
    """
    Log-Cosh Dice Loss for better gradient stability
    """
    def __init__(self, smooth=1e-6):
        super().__init__()
        self.smooth = smooth
    
    def forward(self, pred, target):
        # Calculate Dice score
        pred = pred.view(-1)
        target = target.view(-1)
        
        intersection = (pred * target).sum()
        union = pred.sum() + target.sum()
        dice_score = (2.0 * intersection + self.smooth) / (union + self.smooth)
        
        # Log-Cosh transformation (numerically stable)
        dice_loss = 1.0 - dice_score
        x = dice_loss
        log_cosh = x + torch.nn.functional.softplus(-2.0 * x) - math.log(2.0)
        
        return log_cosh
```

#### 3C. Update Config
**‡πÑ‡∏ü‡∏•‡πå:** `config.py`

```python
# Augmentation
AUG_GAMMA_CORRECTION_PROB = 0.5
AUG_GAMMA_LIMIT = (0.7, 1.5)

# Loss function
LOSS_TYPE = 'log_cosh_dice'  # NEW
```

### ‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå‡∏ó‡∏µ‡πà‡∏Ñ‡∏≤‡∏î‡∏´‡∏ß‡∏±‡∏á:
- **Val Dice: +2-3%**
- **‡πÅ‡∏Å‡πâ‡∏õ‡∏±‡∏ç‡∏´‡∏≤ NaN loss**
- **Training smoother**
- **Better on small lesions**

---

## üîÑ **PHASE 4: Deep Supervision**

### ‡∏™‡∏ñ‡∏≤‡∏ô‡∏∞: üîÑ ‡∏û‡∏£‡πâ‡∏≠‡∏°‡∏î‡∏≥‡πÄ‡∏ô‡∏¥‡∏ô‡∏Å‡∏≤‡∏£

### ‡πÅ‡∏ú‡∏ô‡∏Å‡∏≤‡∏£:

#### 4A. Deep Supervision Architecture
**‡πÑ‡∏ü‡∏•‡πå:** `models/attention_unet.py`

```python
class AttentionUNetDeepSupervision(nn.Module):
    """
    Attention U-Net with Deep Supervision
    """
    def __init__(self, config):
        super().__init__()
        # ... existing encoder/decoder ...
        
        # Auxiliary output heads
        self.aux_head_1 = nn.Conv2d(decoder_channels[0], out_channels, 1)
        self.aux_head_2 = nn.Conv2d(decoder_channels[1], out_channels, 1)
        self.aux_head_3 = nn.Conv2d(decoder_channels[2], out_channels, 1)
        self.final_head = nn.Conv2d(decoder_channels[3], out_channels, 1)
        
        # Deep supervision weights
        self.ds_weights = [0.1, 0.2, 0.3, 0.4]
    
    def forward(self, x, return_auxiliary=False):
        # Encoder
        enc1, x = self.encoder1(x)
        enc2, x = self.encoder2(x)
        enc3, x = self.encoder3(x)
        enc4, x = self.encoder4(x)
        
        # Bottleneck
        x = self.bottleneck(x)
        
        # Decoder with auxiliary outputs
        dec4 = self.decoder4(x, enc4)
        out_aux1 = torch.sigmoid(self.aux_head_1(dec4))
        
        dec3 = self.decoder3(dec4, enc3)
        out_aux2 = torch.sigmoid(self.aux_head_2(dec3))
        
        dec2 = self.decoder2(dec3, enc2)
        out_aux3 = torch.sigmoid(self.aux_head_3(dec2))
        
        dec1 = self.decoder1(dec2, enc1)
        out_final = torch.sigmoid(self.final_head(dec1))
        
        if return_auxiliary:
            return {
                'aux1': out_aux1,
                'aux2': out_aux2,
                'aux3': out_aux3,
                'final': out_final
            }
        else:
            return out_final
```

#### 4B. Deep Supervision Loss
**‡πÑ‡∏ü‡∏•‡πå:** `loss.py`

```python
class DeepSupervisionLoss(nn.Module):
    """
    Deep Supervision Loss wrapper
    """
    def __init__(self, base_loss, weights=[0.1, 0.2, 0.3, 0.4]):
        super().__init__()
        self.base_loss = base_loss
        self.weights = weights
    
    def forward(self, outputs, target):
        total_loss = 0
        
        for key, weight in zip(['aux1', 'aux2', 'aux3', 'final'], 
                               self.weights):
            output = outputs[key]
            
            # Resize target if needed
            if output.shape != target.shape:
                target_resized = F.interpolate(
                    target, 
                    size=output.shape[2:], 
                    mode='nearest'
                )
            else:
                target_resized = target
            
            # Calculate weighted loss
            loss = self.base_loss(output, target_resized)
            total_loss += weight * loss
        
        return total_loss
```

#### 4C. Update Training
**‡πÑ‡∏ü‡∏•‡πå:** `config.py`

```python
# Deep Supervision
USE_DEEP_SUPERVISION = True
DS_WEIGHTS = [0.1, 0.2, 0.3, 0.4]  # Sum = 1.0
DS_DISABLE_AFTER_EPOCH = 50  # Optional: disable auxiliary heads after N epochs
```

### ‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå‡∏ó‡∏µ‡πà‡∏Ñ‡∏≤‡∏î‡∏´‡∏ß‡∏±‡∏á:
- **Val Dice: +2-4%**
- **Training converge ‡πÄ‡∏£‡πá‡∏ß‡∏Ç‡∏∂‡πâ‡∏ô 20-30%**
- **Better gradient flow**
- **Small lesion detection ‡∏î‡∏µ‡∏Ç‡∏∂‡πâ‡∏ô +5-10%**

---

## üìÖ **Timeline ‡πÅ‡∏ô‡∏∞‡∏ô‡∏≥**

### ‡∏™‡∏±‡∏õ‡∏î‡∏≤‡∏´‡πå‡∏ó‡∏µ‡πà 1: Quick Wins (Phase 1-2)
- ‚úÖ **‡∏ß‡∏±‡∏ô‡∏ó‡∏µ‡πà 1-2**: Phase 1 - TTA + CCA (‡πÄ‡∏™‡∏£‡πá‡∏à‡πÅ‡∏•‡πâ‡∏ß)
- ‚úÖ **‡∏ß‡∏±‡∏ô‡∏ó‡∏µ‡πà 3-4**: Phase 2 - N4 Correction (‡πÄ‡∏™‡∏£‡πá‡∏à‡πÅ‡∏•‡πâ‡∏ß)
- üîÑ **‡∏ß‡∏±‡∏ô‡∏ó‡∏µ‡πà 5**: ‡∏ó‡∏î‡∏™‡∏≠‡∏ö evaluation ‡∏Å‡∏±‡∏ö best_model.pth ‡∏õ‡∏±‡∏à‡∏à‡∏∏‡∏ö‡∏±‡∏ô

### ‡∏™‡∏±‡∏õ‡∏î‡∏≤‡∏´‡πå‡∏ó‡∏µ‡πà 2: Training Improvements (Phase 3-4)
- üîÑ **‡∏ß‡∏±‡∏ô‡∏ó‡∏µ‡πà 6-7**: Phase 3 - Gamma + Log-Cosh Loss
- üîÑ **‡∏ß‡∏±‡∏ô‡∏ó‡∏µ‡πà 8-9**: Re-train model ‡∏î‡πâ‡∏ß‡∏¢ preprocessing ‡πÅ‡∏•‡∏∞ augmentation ‡πÉ‡∏´‡∏°‡πà
- üîÑ **‡∏ß‡∏±‡∏ô‡∏ó‡∏µ‡πà 10**: Phase 4 - Deep Supervision (optional)

### ‡∏™‡∏±‡∏õ‡∏î‡∏≤‡∏´‡πå‡∏ó‡∏µ‡πà 3: Fine-tuning & Analysis
- üîÑ **‡∏ß‡∏±‡∏ô‡∏ó‡∏µ‡πà 11-12**: Train model ‡∏î‡πâ‡∏ß‡∏¢ Deep Supervision
- üîÑ **‡∏ß‡∏±‡∏ô‡∏ó‡∏µ‡πà 13-14**: Hyperparameter tuning
- üîÑ **‡∏ß‡∏±‡∏ô‡∏ó‡∏µ‡πà 15**: Final evaluation ‡πÅ‡∏•‡∏∞ comparison

---

## üéØ **‡∏Å‡∏≤‡∏£‡∏ó‡∏î‡∏™‡∏≠‡∏ö Phase 1-2 (‡∏Ç‡∏±‡πâ‡∏ô‡∏ï‡∏≠‡∏ô‡∏ñ‡∏±‡∏î‡πÑ‡∏õ)**

### ‡∏Å‡πà‡∏≠‡∏ô Re-train: ‡∏ó‡∏î‡∏™‡∏≠‡∏ö TTA+CCA ‡∏Å‡∏±‡∏ö model ‡∏õ‡∏±‡∏à‡∏à‡∏∏‡∏ö‡∏±‡∏ô

```bash
# 1. ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö config
python -c "import config; print(f'TTA: {config.USE_TTA}'); print(f'CCA: {config.USE_CCA}')"

# 2. Run evaluation (‡πÉ‡∏ä‡πâ model ‡∏õ‡∏±‡∏à‡∏à‡∏∏‡∏ö‡∏±‡∏ô + TTA/CCA)
python evaluate.py

# 3. ‡πÄ‡∏õ‡∏£‡∏µ‡∏¢‡∏ö‡πÄ‡∏ó‡∏µ‡∏¢‡∏ö‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå
# - ‡∏î‡∏π test_per_sample_results.csv
# - ‡πÄ‡∏õ‡∏£‡∏µ‡∏¢‡∏ö‡πÄ‡∏ó‡∏µ‡∏¢‡∏ö Dice score ‡∏Å‡πà‡∏≠‡∏ô/‡∏´‡∏•‡∏±‡∏á TTA+CCA
```

### ‡∏´‡∏•‡∏±‡∏á‡∏ó‡∏î‡∏™‡∏≠‡∏ö TTA+CCA ‡∏™‡∏≥‡πÄ‡∏£‡πá‡∏à: Re-preprocess ‡∏î‡πâ‡∏ß‡∏¢ N4

```bash
# 1. Install SimpleITK
pip install SimpleITK

# 2. Backup ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÄ‡∏î‡∏¥‡∏°
mv 2_data_processed 2_data_processed_backup

# 3. Run preprocessing ‡πÉ‡∏´‡∏°‡πà (‡πÉ‡∏ä‡πâ‡πÄ‡∏ß‡∏•‡∏≤ ~30-60 ‡∏ô‡∏≤‡∏ó‡∏µ)
python 01_preprocess.py

# 4. ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå
# - ‡πÄ‡∏õ‡∏£‡∏µ‡∏¢‡∏ö‡πÄ‡∏ó‡∏µ‡∏¢‡∏ö mean/std ‡∏Å‡πà‡∏≠‡∏ô/‡∏´‡∏•‡∏±‡∏á N4
# - ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö preprocess_config.json
```

### Re-train Model ‡∏î‡πâ‡∏ß‡∏¢‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÉ‡∏´‡∏°‡πà

```bash
# Train ‡∏î‡πâ‡∏ß‡∏¢ data ‡∏ó‡∏µ‡πà‡∏°‡∏µ N4 correction
python train.py

# Expected: Val Dice ‡πÄ‡∏û‡∏¥‡πà‡∏°‡∏Ç‡∏∂‡πâ‡∏ô +3-6%
```

---

## üìä **Expected Final Results**

### Baseline (Current)
```
Val Dice:  70%
Test Dice: 62%
Gap:       8%
```

### After Phase 1 (TTA+CCA only - no retrain)
```
Val Dice:  70% (unchanged)
Test Dice: 64-66% (+2-4%)
Gap:       4-6%
```

### After Phase 1+2 (TTA+CCA + N4 + retrain)
```
Val Dice:  73-76% (+3-6%)
Test Dice: 67-72% (+5-10%)
Gap:       2-6%
```

### After Phase 1+2+3 (+ Gamma + Log-Cosh)
```
Val Dice:  74-77% (+4-7%)
Test Dice: 69-75% (+7-13%)
Gap:       1-5%
```

### After All Phases (+ Deep Supervision)
```
Val Dice:  75-78% (+5-8%)
Test Dice: 71-79% (+9-17%) ‚≠ê
Gap:       1-4%
```

---

## ‚ö†Ô∏è **‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏£‡∏£‡∏∞‡∏ß‡∏±‡∏á**

### Phase 1-2 (‡πÄ‡∏™‡∏£‡πá‡∏à‡πÅ‡∏•‡πâ‡∏ß)
- ‚úÖ N4 correction ‡∏ï‡πâ‡∏≠‡∏á install SimpleITK
- ‚úÖ Re-preprocessing ‡∏à‡∏∞‡πÉ‡∏ä‡πâ‡πÄ‡∏ß‡∏•‡∏≤ 30-60 ‡∏ô‡∏≤‡∏ó‡∏µ
- ‚úÖ ‡∏ï‡πâ‡∏≠‡∏á backup ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÄ‡∏î‡∏¥‡∏°‡∏Å‡πà‡∏≠‡∏ô re-preprocess

### Phase 3 (‡∏¢‡∏±‡∏á‡πÑ‡∏°‡πà‡πÑ‡∏î‡πâ‡∏ó‡∏≥)
- ‚ö†Ô∏è Gamma correction ‡∏≠‡∏≤‡∏à‡∏ó‡∏≥‡πÉ‡∏´‡πâ training ‡∏ä‡πâ‡∏≤‡∏•‡∏á ~5%
- ‚ö†Ô∏è Log-Cosh Loss ‡∏ï‡πâ‡∏≠‡∏á‡∏ó‡∏î‡∏™‡∏≠‡∏ö numerical stability

### Phase 4 (‡∏¢‡∏±‡∏á‡πÑ‡∏°‡πà‡πÑ‡∏î‡πâ‡∏ó‡∏≥)
- ‚ö†Ô∏è Deep Supervision ‡πÉ‡∏ä‡πâ memory ‡πÄ‡∏û‡∏¥‡πà‡∏° ~20%
- ‚ö†Ô∏è ‡∏≠‡∏≤‡∏à‡∏ï‡πâ‡∏≠‡∏á‡∏•‡∏î batch size ‡∏à‡∏≤‡∏Å 16 ‚Üí 12
- ‚ö†Ô∏è Training ‡∏ä‡πâ‡∏≤‡∏•‡∏á ~15-20%

---

## üìù **‡∏´‡∏°‡∏≤‡∏¢‡πÄ‡∏´‡∏ï‡∏∏‡∏™‡∏≥‡∏Ñ‡∏±‡∏ç**

### ‡πÑ‡∏ü‡∏•‡πå‡∏ó‡∏µ‡πà‡πÅ‡∏Å‡πâ‡πÑ‡∏Ç‡πÅ‡∏•‡πâ‡∏ß (Phase 1-2):
1. ‚úÖ `evaluation_module.py` - ‡πÄ‡∏û‡∏¥‡πà‡∏° TTA ‡πÅ‡∏•‡∏∞ CCA functions
2. ‚úÖ `evaluate.py` - ‡∏ú‡∏™‡∏≤‡∏ô TTA+CCA
3. ‚úÖ `config.py` - ‡πÄ‡∏û‡∏¥‡πà‡∏° TTA, CCA, N4 parameters
4. ‚úÖ `01_preprocess.py` - ‡πÄ‡∏û‡∏¥‡πà‡∏° N4 correction

### ‡πÑ‡∏ü‡∏•‡πå‡∏ó‡∏µ‡πà‡∏à‡∏∞‡πÅ‡∏Å‡πâ‡πÑ‡∏Ç (Phase 3-4):
- üîÑ `dataset.py` - ‡πÄ‡∏û‡∏¥‡πà‡∏° RandomGamma
- üîÑ `loss.py` - ‡πÄ‡∏û‡∏¥‡πà‡∏° LogCoshDiceLoss ‡πÅ‡∏•‡∏∞ DeepSupervisionLoss
- üîÑ `models/attention_unet.py` - ‡πÄ‡∏û‡∏¥‡πà‡∏° Deep Supervision
- üîÑ `config.py` - ‡πÄ‡∏û‡∏¥‡πà‡∏° parameters ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö Phase 3-4

### ‡∏Å‡∏≤‡∏£ Rollback:
```bash
# Rollback preprocessing
rm -rf 2_data_processed
mv 2_data_processed_backup 2_data_processed

# Rollback code (‡∏ñ‡πâ‡∏≤‡πÉ‡∏ä‡πâ git)
git checkout config.py
git checkout evaluation_module.py
git checkout evaluate.py
git checkout 01_preprocess.py
```

---

## üéì **‡∏ö‡∏ó‡πÄ‡∏£‡∏µ‡∏¢‡∏ô‡πÅ‡∏•‡∏∞ Best Practices**

### 1. N4 Bias Correction
- ‡∏ó‡∏≥ **‡∏Å‡πà‡∏≠‡∏ô** resize ‡πÄ‡∏™‡∏°‡∏≠ (‡πÑ‡∏î‡πâ‡∏ú‡∏•‡∏î‡∏µ‡∏Å‡∏ß‡πà‡∏≤)
- ‡∏õ‡∏¥‡∏î CLAHE ‡πÄ‡∏°‡∏∑‡πà‡∏≠‡πÉ‡∏ä‡πâ N4 (‡πÑ‡∏°‡πà‡∏à‡∏≥‡πÄ‡∏õ‡πá‡∏ô redundant)
- shrink_factor=4 ‡πÄ‡∏õ‡πá‡∏ô sweet spot (‡πÄ‡∏£‡πá‡∏ß ‡∏Ñ‡∏∏‡∏ì‡∏†‡∏≤‡∏û‡∏î‡∏µ)

### 2. Test-Time Augmentation
- `['hflip', 'vflip']` ‡πÄ‡∏û‡∏µ‡∏¢‡∏á‡∏û‡∏≠ (2x slower, ‡∏î‡∏µ‡πÅ‡∏•‡πâ‡∏ß)
- ‡πÄ‡∏û‡∏¥‡πà‡∏° rotations ‡πÑ‡∏î‡πâ‡∏ú‡∏•‡∏î‡∏µ‡∏Ç‡∏∂‡πâ‡∏ô‡πÄ‡∏•‡πá‡∏Å‡∏ô‡πâ‡∏≠‡∏¢ ‡πÅ‡∏ï‡πà‡∏ä‡πâ‡∏≤‡∏°‡∏≤‡∏Å
- ‡πÉ‡∏ä‡πâ‡πÉ‡∏ô final evaluation ‡πÄ‡∏ó‡πà‡∏≤‡∏ô‡∏±‡πâ‡∏ô (‡πÑ‡∏°‡πà‡πÉ‡∏ä‡πâ‡πÉ‡∏ô validation loop)

### 3. Connected Component Analysis
- `min_size=10` pixels ‡πÄ‡∏´‡∏°‡∏≤‡∏∞‡∏Å‡∏±‡∏ö 4mm spacing
- `min_confidence=0.3` ‡∏Å‡∏£‡∏≠‡∏á‡πÑ‡∏î‡πâ‡∏î‡∏µ ‡πÑ‡∏°‡πà‡πÄ‡∏Å‡∏¥‡∏ô‡πÄ‡∏•‡∏¢
- ‡∏ï‡πâ‡∏≠‡∏á tune ‡∏ï‡∏≤‡∏° dataset (‡∏ñ‡πâ‡∏≤ lesion ‡πÄ‡∏•‡πá‡∏Å‡∏°‡∏≤‡∏Å ‡∏≠‡∏≤‡∏à‡∏•‡∏î min_size)

### 4. Training Order
1. TTA+CCA ‡∏Å‡πà‡∏≠‡∏ô (‡πÑ‡∏°‡πà‡∏ï‡πâ‡∏≠‡∏á retrain)
2. N4 correction (retrain ‡∏Ñ‡∏£‡∏±‡πâ‡∏á‡πÅ‡∏£‡∏Å)
3. Augmentation + Loss (retrain ‡∏Ñ‡∏£‡∏±‡πâ‡∏á‡∏ó‡∏µ‡πà‡∏™‡∏≠‡∏á)
4. Deep Supervision (retrain ‡∏Ñ‡∏£‡∏±‡πâ‡∏á‡∏™‡∏∏‡∏î‡∏ó‡πâ‡∏≤‡∏¢)

---

## üîó **References**

### Papers:
1. N4ITK: Tustison et al., "N4ITK: Improved N3 Bias Correction", IEEE TMI 2010
2. TTA: https://arxiv.org/abs/1511.00561
3. Deep Supervision: https://arxiv.org/abs/1807.10165
4. Log-Cosh Loss: https://arxiv.org/abs/1810.00382

### Libraries:
- SimpleITK: https://simpleitk.org/
- Albumentations: https://albumentations.ai/
- scikit-image: https://scikit-image.org/

---

**‡∏™‡∏£‡πâ‡∏≤‡∏á‡πÇ‡∏î‡∏¢:** GitHub Copilot  
**‡∏ß‡∏±‡∏ô‡∏ó‡∏µ‡πà:** 27 ‡∏û‡∏§‡∏®‡∏à‡∏¥‡∏Å‡∏≤‡∏¢‡∏ô 2568  
**‡πÄ‡∏ß‡∏≠‡∏£‡πå‡∏ä‡∏±‡∏ô:** 1.0
