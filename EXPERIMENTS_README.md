# üß™ Automated Experiment System

‡∏£‡∏∞‡∏ö‡∏ö‡∏£‡∏±‡∏ô‡∏Å‡∏≤‡∏£‡∏ó‡∏î‡∏•‡∏≠‡∏á‡∏≠‡∏±‡∏ï‡πÇ‡∏ô‡∏°‡∏±‡∏ï‡∏¥‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö DWI Segmentation Project

## üìÅ ‡πÑ‡∏ü‡∏•‡πå‡∏ó‡∏µ‡πà‡πÄ‡∏Å‡∏µ‡πà‡∏¢‡∏ß‡∏Ç‡πâ‡∏≠‡∏á

- `run_experiments.py` - Main script ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏£‡∏±‡∏ô‡∏Å‡∏≤‡∏£‡∏ó‡∏î‡∏•‡∏≠‡∏á
- `analyze_results.py` - ‡∏ß‡∏¥‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå‡πÅ‡∏•‡∏∞‡∏™‡∏£‡∏∏‡∏õ‡∏ú‡∏•‡∏Å‡∏≤‡∏£‡∏ó‡∏î‡∏•‡∏≠‡∏á
- `experiments_example.json` - ‡∏ï‡∏±‡∏ß‡∏≠‡∏¢‡πà‡∏≤‡∏á custom experiment config
- `experiment_results.json` - ‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î (auto-generated)
- `experiment_logs/` - Log files ‡πÅ‡∏ï‡πà‡∏•‡∏∞‡∏Å‡∏≤‡∏£‡∏ó‡∏î‡∏•‡∏≠‡∏á

## üöÄ Quick Start

### Stage 1: Architecture Selection (16 experiments, ~8 hours)
‡∏ó‡∏î‡∏™‡∏≠‡∏ö 6 architectures √ó 3 encoders ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏´‡∏≤ best combination

```bash
python run_experiments.py --stage 1
```

**Experiments:**
- attention_unet (custom)
- unet++ √ó 3 encoders (efficientnet-b0, efficientnet-b3, resnet34)
- fpn √ó 3 encoders
- deeplabv3+ √ó 3 encoders
- manet √ó 3 encoders
- pspnet √ó 3 encoders

**Expected time:** ~30 min/experiment √ó 16 = ~8 hours

### Stage 2: Resolution Optimization (9 experiments, ~5 hours)
‡∏ó‡∏î‡∏™‡∏≠‡∏ö 3 resolutions ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö top-3 models

```bash
# ‡∏î‡∏π‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå stage 1 ‡∏Å‡πà‡∏≠‡∏ô
python analyze_results.py

# ‡πÄ‡∏•‡∏∑‡∏≠‡∏Å top-3 models ‡πÅ‡∏•‡πâ‡∏ß‡∏£‡∏±‡∏ô stage 2
python run_experiments.py --stage 2 --top-models "manet_efficientnet-b3,deeplabv3+_resnet34,fpn_efficientnet-b3"
```

**Experiments:**
- Top 3 models √ó 3 resolutions (256, 384, 512)

**Expected time:** ~30-40 min/experiment √ó 9 = ~5 hours

### Stage 3: Preprocessing (4 experiments, ~2 hours)
‡∏ó‡∏î‡∏™‡∏≠‡∏ö CLAHE ‡πÅ‡∏•‡∏∞ Augmentation

```bash
python run_experiments.py --stage 3
```

**Experiments:**
- Best model √ó (CLAHE on/off) √ó (Aug on/off)

**Expected time:** ~30 min/experiment √ó 4 = ~2 hours

### Stage 4: Fine-tuning (9 experiments, ~4-5 hours)
‡∏ó‡∏î‡∏™‡∏≠‡∏ö loss functions ‡πÅ‡∏•‡∏∞ learning rates

```bash
python run_experiments.py --stage 4
```

**Experiments:**
- Best model √ó 3 loss types √ó 3 learning rates

**Expected time:** ~30 min/experiment √ó 9 = ~4-5 hours

---

## üìä ‡∏Å‡∏≤‡∏£‡∏ß‡∏¥‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå

### ‡∏î‡∏π‡∏™‡∏£‡∏∏‡∏õ‡∏ú‡∏•
```bash
python analyze_results.py
```

Output:
- Summary statistics
- Top 10 results
- Architecture comparison
- Encoder comparison
- Resolution comparison
- Preprocessing impact

### Export ‡πÄ‡∏õ‡πá‡∏ô CSV
```bash
python analyze_results.py --export-csv results.csv
```

### Export ‡πÄ‡∏õ‡πá‡∏ô HTML Report
```bash
python analyze_results.py --export-html report.html
```

### ‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏Å‡∏£‡∏≤‡∏ü‡πÄ‡∏õ‡∏£‡∏µ‡∏¢‡∏ö‡πÄ‡∏ó‡∏µ‡∏¢‡∏ö
```bash
python analyze_results.py --plot
```

‡πÑ‡∏ü‡∏•‡πå‡∏Å‡∏£‡∏≤‡∏ü‡∏à‡∏∞‡∏ñ‡∏π‡∏Å‡∏™‡∏£‡πâ‡∏≤‡∏á‡πÉ‡∏ô `experiment_plots/`:
- `architecture_comparison.png`
- `encoder_comparison.png`
- `time_vs_performance.png`
- `resolution_comparison.png`

---

## üé® Custom Experiments

### ‡∏™‡∏£‡πâ‡∏≤‡∏á Custom Config

‡∏™‡∏£‡πâ‡∏≤‡∏á‡πÑ‡∏ü‡∏•‡πå JSON (‡πÄ‡∏ä‡πà‡∏ô `my_experiments.json`):

```json
{
  "description": "My custom experiments",
  "experiments": [
    {
      "id": "exp1_manet_b3_512",
      "params": {
        "MODEL_ARCHITECTURE": "manet",
        "ENCODER_NAME": "efficientnet-b3",
        "IMAGE_SIZE": [512, 512],
        "BATCH_SIZE": 8,
        "NUM_EPOCHS": 200,
        "AUGMENTATION_ENABLED": true,
        "CLAHE_ENABLED": false,
        "LEARNING_RATE": 8e-5,
        "LOSS_TYPE": "dice"
      }
    },
    {
      "id": "exp2_attention_unet_256",
      "params": {
        "MODEL_ARCHITECTURE": "attention_unet",
        "IMAGE_SIZE": [256, 256],
        "BATCH_SIZE": 32,
        "NUM_EPOCHS": 200,
        "AUGMENTATION_ENABLED": true,
        "CLAHE_ENABLED": true
      }
    }
  ]
}
```

### ‡∏£‡∏±‡∏ô Custom Config

```bash
python run_experiments.py --config my_experiments.json
```

---

## üîß Advanced Options

### Skip Completed Experiments
```bash
python run_experiments.py --stage 1 --skip-existing
```

### Continue Failed Experiments
```bash
# ‡πÅ‡∏Å‡πâ‡πÑ‡∏Ç‡∏õ‡∏±‡∏ç‡∏´‡∏≤‡πÅ‡∏•‡πâ‡∏ß‡∏£‡∏±‡∏ô‡πÉ‡∏´‡∏°‡πà (‡∏à‡∏∞ skip experiments ‡∏ó‡∏µ‡πà‡∏™‡∏≥‡πÄ‡∏£‡πá‡∏à‡πÅ‡∏•‡πâ‡∏ß)
python run_experiments.py --stage 1 --skip-existing
```

---

## üìù Experiment Parameters

### ‡∏Ñ‡πà‡∏≤‡∏ó‡∏µ‡πà‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡∏õ‡∏£‡∏±‡∏ö‡πÑ‡∏î‡πâ:

**Model Architecture:**
- `MODEL_ARCHITECTURE`: attention_unet, unet++, fpn, deeplabv3+, manet, pspnet

**Encoder (‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö SMP models):**
- `ENCODER_NAME`: efficientnet-b0, efficientnet-b3, resnet34, resnet50, resnext50_32x4d

**Image Processing:**
- `IMAGE_SIZE`: (256,256), (384,384), (512,512)
- `CLAHE_ENABLED`: true/false
- `AUGMENTATION_ENABLED`: true/false

**Training:**
- `NUM_EPOCHS`: 200 (default), 100 (quick test)
- `BATCH_SIZE`: 8, 16, 32
- `LEARNING_RATE`: 5e-5, 8e-5, 1e-4
- `OPTIMIZER`: adam, adamw
- `LOSS_TYPE`: dice, focal, combo

**Regularization:**
- `WEIGHT_DECAY`: 1e-5, 8e-5, 1e-4
- `GRADIENT_CLIP_VALUE`: 1.0

---

## üìà Expected Results

### Total Experiments: ~38 experiments
- Stage 1: 16 experiments (~8 hours)
- Stage 2: 9 experiments (~5 hours)  
- Stage 3: 4 experiments (~2 hours)
- Stage 4: 9 experiments (~5 hours)

**Total time: ~20 hours** (‡∏ñ‡πâ‡∏≤‡∏£‡∏±‡∏ô‡∏ó‡∏µ‡∏•‡∏∞‡∏ï‡∏±‡∏ß)

### Best Practices:

1. **Run Stage 1 first** - ‡∏´‡∏≤ best architecture
2. **Analyze results** - ‡πÄ‡∏•‡∏∑‡∏≠‡∏Å top-3 models
3. **Run Stage 2** - optimize resolution
4. **Run Stage 3** - optimize preprocessing
5. **Run Stage 4** - fine-tune hyperparameters

---

## üêõ Troubleshooting

### Experiment Failed
- ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö log file: `experiment_logs/{experiment_id}.log`
- ‡πÅ‡∏Å‡πâ‡πÑ‡∏Ç‡∏õ‡∏±‡∏ç‡∏´‡∏≤‡πÅ‡∏•‡πâ‡∏ß‡∏£‡∏±‡∏ô‡πÉ‡∏´‡∏°‡πà (‡∏à‡∏∞ skip experiments ‡∏ó‡∏µ‡πà‡∏™‡∏≥‡πÄ‡∏£‡πá‡∏à‡πÅ‡∏•‡πâ‡∏ß)

### Out of Memory
- ‡∏•‡∏î `BATCH_SIZE` ‡πÉ‡∏ô‡∏Å‡∏≤‡∏£‡∏ó‡∏î‡∏•‡∏≠‡∏á
- ‡∏´‡∏£‡∏∑‡∏≠‡∏•‡∏î `IMAGE_SIZE`

### Preprocessing Error
- ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏ß‡πà‡∏≤ raw data ‡∏°‡∏µ‡∏≠‡∏¢‡∏π‡πà‡πÉ‡∏ô `1_data_raw/`
- ‡∏£‡∏±‡∏ô `python 01_preprocess.py` ‡πÅ‡∏¢‡∏Å‡∏Å‡πà‡∏≠‡∏ô

### Training Timeout
- ‡πÄ‡∏û‡∏¥‡πà‡∏° timeout ‡πÉ‡∏ô `run_experiments.py` (default: 2 hours)
- ‡∏´‡∏£‡∏∑‡∏≠‡∏•‡∏î `NUM_EPOCHS` ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏Å‡∏≤‡∏£‡∏ó‡∏î‡∏•‡∏≠‡∏á‡πÄ‡∏£‡πá‡∏ß

---

## üí° Tips

### Quick Test (5 minutes)
‡∏ó‡∏î‡∏™‡∏≠‡∏ö‡∏ß‡πà‡∏≤ system ‡∏ó‡∏≥‡∏á‡∏≤‡∏ô‡πÑ‡∏î‡πâ‡∏Å‡πà‡∏≠‡∏ô‡∏£‡∏±‡∏ô full experiments:

```bash
# ‡∏™‡∏£‡πâ‡∏≤‡∏á quick_test.json
{
  "experiments": [
    {
      "id": "quick_test",
      "params": {
        "MODEL_ARCHITECTURE": "manet",
        "ENCODER_NAME": "efficientnet-b0",
        "IMAGE_SIZE": [256, 256],
        "BATCH_SIZE": 16,
        "NUM_EPOCHS": 2,
        "AUGMENTATION_ENABLED": false
      }
    }
  ]
}

# ‡∏£‡∏±‡∏ô
python run_experiments.py --config quick_test.json
```

### Parallel Experiments
‡∏ñ‡πâ‡∏≤‡∏°‡∏µ multiple GPUs ‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡∏£‡∏±‡∏ô‡∏´‡∏•‡∏≤‡∏¢ experiments ‡∏û‡∏£‡πâ‡∏≠‡∏°‡∏Å‡∏±‡∏ô‡πÑ‡∏î‡πâ:
- ‡πÅ‡∏¢‡∏Å‡πÑ‡∏ü‡∏•‡πå config
- ‡∏£‡∏±‡∏ô‡πÅ‡∏ï‡πà‡∏•‡∏∞‡πÑ‡∏ü‡∏•‡πå‡∏ö‡∏ô GPU ‡∏ï‡πà‡∏≤‡∏á‡∏Å‡∏±‡∏ô
- ‡πÉ‡∏ä‡πâ `CUDA_VISIBLE_DEVICES=0 python run_experiments.py ...`

---

## üì¶ Output Files

```
dwi-t3-training/
‚îú‚îÄ‚îÄ experiment_results.json      # ‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î (JSON)
‚îú‚îÄ‚îÄ experiment_logs/             # Log ‡πÅ‡∏ï‡πà‡∏•‡∏∞‡∏Å‡∏≤‡∏£‡∏ó‡∏î‡∏•‡∏≠‡∏á
‚îÇ   ‚îú‚îÄ‚îÄ s1_manet_efficientnet-b3.log
‚îÇ   ‚îú‚îÄ‚îÄ s1_deeplabv3+_resnet34.log
‚îÇ   ‚îî‚îÄ‚îÄ ...
‚îú‚îÄ‚îÄ experiment_plots/            # ‡∏Å‡∏£‡∏≤‡∏ü‡πÄ‡∏õ‡∏£‡∏µ‡∏¢‡∏ö‡πÄ‡∏ó‡∏µ‡∏¢‡∏ö
‚îÇ   ‚îú‚îÄ‚îÄ architecture_comparison.png
‚îÇ   ‚îú‚îÄ‚îÄ encoder_comparison.png
‚îÇ   ‚îî‚îÄ‚îÄ ...
‚îî‚îÄ‚îÄ mlruns/                      # MLflow tracking
    ‚îî‚îÄ‚îÄ {experiment_id}/
```

---

## üéØ Goal

‡∏´‡∏≤ best configuration ‡∏ó‡∏µ‡πà‡πÉ‡∏´‡πâ:
- **Highest Dice Score** (validation + test)
- **Reasonable Training Time** (<1 hour)
- **Stable Performance** (low variance)
- **Good Generalization** (val-test gap < 5%)

---

## üìö Additional Resources

- MLflow UI: `mlflow ui --backend-store-uri ./mlruns`
- Config reference: `config.py`
- Model architectures: `models.py`
- Dataset info: `dataset.py`

---

**Happy Experimenting! üöÄ**
